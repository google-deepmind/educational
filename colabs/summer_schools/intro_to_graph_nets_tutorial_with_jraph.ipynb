{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8WeEpdX4Vx2"
      },
      "source": [
        "# Introduction to Graph Neural Nets with JAX/jraph\n",
        "\n",
        "*Lisa Wang, DeepMind (wanglisa@deepmind.com), Nikola Jovanoviƒá, ETH Zurich (nikola.jovanovic@inf.ethz.ch)*\n",
        "\n",
        "**Colab Runtime:**\n",
        "\n",
        "If possible, please use a GPU hardware accelerator to run this colab. You can choose that under *Runtime \u003e Change Runtime Type*.\n",
        "\n",
        "**Prerequisites:**\n",
        "* Some familiarity with [JAX](https://github.com/google/jax), you can refer to this [colab](https://colab.sandbox.google.com/github/google/jax/blob/master/docs/jax-101/01-jax-basics.ipynb) for an introduction to JAX.\n",
        "* Neural network basics\n",
        "* Graph theory basics (MIT Open Courseware [slides](https://ocw.mit.edu/courses/civil-and-environmental-engineering/1-022-introduction-to-network-models-fall-2018/lecture-notes/MIT1_022F18_lec2.pdf) by Amir Ajorlou)\n",
        "\n",
        "\n",
        "We recommend watching the [Theoretical Foundations of Graph Neural Networks Lecture](https://www.youtube.com/watch?v=uF53xsT7mjc\u0026) by Petar Veliƒçkoviƒá before working through this colab. The talk provides a theoretical introduction to Graph Neural Networks (GNNs), historical context and motivating examples.\n",
        "\n",
        "\n",
        "**Outline:**\n",
        "* [Fundamental Graph Concepts](#scrollTo=gsKA-syx_LUi)\n",
        "* [Graph Prediction Tasks](#scrollTo=spQGRxhPN8Eo)\n",
        "* [Intro to the jraph Library](#scrollTo=3C5YI9M0vwvb)\n",
        "* [Graph Convolutional Network (GCN) Layer](#scrollTo=NZRMF2d-h2pd)\n",
        "* [Build GCN Model with Multiple Layers](#scrollTo=lha8rbQ78l3S)\n",
        "* [Node Classification with GCN on Karate Club Dataset](#scrollTo=Z5t7kw7SE_h4)\n",
        "* [Graph Attention (GAT) Layer](#scrollTo=yg8g96NdBCK6)\n",
        "* [Train GAT Model on Karate Club Dataset](#scrollTo=anfVGJwBe27v)\n",
        "* [Graph Classification on MUTAG (Molecules)](#scrollTo=n5TxaTGzBkBa)\n",
        "* [Link Prediction on CORA (Citation Network)](#scrollTo=OwVE88dTRC6V)\n",
        "* [Bonus: Intro to Graph Adversarial Attacks](#scrollTo=35kbP8GZRFEm)\n",
        "\n",
        "\n",
        "**Additional Resources:**\n",
        "\n",
        "* Battaglia et al. (2018): [Relational inductive biases, deep learning, and graph networks](https://arxiv.org/pdf/1806.01261)\n",
        "\n",
        "---\n",
        "\n",
        "Some sections in this colab build on the [GraphNets Tutorial colab in pytorch](https://github.com/eemlcommunity/PracticalSessions2021/blob/main/graphnets/graphnets_tutorial.ipynb) by Nikola Jovanoviƒá.\n",
        "\n",
        "We would like to thank Razvan Pascanu and Petar Veliƒçkoviƒá for their valuable input and feedback.\n",
        "\n",
        "---\n",
        "*Copyright 2022 by the Authors.*\n",
        "\n",
        "*Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0*\n",
        "\n",
        "*Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUxD4gOd-AuO"
      },
      "source": [
        "## Setup: Install and Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyH_VgjW7Bp3"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/deepmind/jraph.git\n",
        "!pip install flax\n",
        "!pip install dm-haiku"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJm7y6GH3WyB"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "%matplotlib inline\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.tree_util as tree\n",
        "import jraph\n",
        "import flax\n",
        "import haiku as hk\n",
        "import optax\n",
        "import pickle\n",
        "import numpy as onp\n",
        "import networkx as nx\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsKA-syx_LUi"
      },
      "source": [
        "## Fundamental Graph Concepts\n",
        "A graph consists of a set of nodes and a set of edges, where edges form connections between nodes.\n",
        "\n",
        "More formally, a graph is defined as $ \\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ where $\\mathcal{V}$ is the set of vertices / nodes, and $\\mathcal{E}$ is the set of edges.\n",
        "\n",
        "In an **undirected** graph, each edge is an unordered pair of two nodes $ \\in \\mathcal{V}$. E.g. a friend network can be represented as an undirected graph, assuming that the relationship \"*A is friends with B*\" implies \"*B is friends with A*\".\n",
        "\n",
        "In a **directed** graph, each edge is an ordered pair of nodes $ \\in \\mathcal{V}$. E.g. a citation network would be best represented with a directed graph, since the relationship \"*A cites B*\" does not imply \"*B cites A*\".\n",
        "\n",
        "The **degree** of a node is defined as the number of edges incident on it, i.e. the sum of incoming and outgoing edges for that node.\n",
        "\n",
        "The **in-degree** is the sum of incoming edges only, and the **out-degree** is the sum of outgoing edges only.\n",
        "\n",
        "There are several ways to represent $\\mathcal{E}$:\n",
        "1. As a **list of edges**: a list of pairs $(u,v)$, where $(u,v)$ means that there is an edge going from node $u$ to node $v$.\n",
        "2. As an **adjacency matrix**: a binary square matrix $A$ of size $|\\mathcal{V}| \\times |\\mathcal{V}|$, where $A_{u,v}=1$ iff there is a connection between nodes $u$ and $v$.\n",
        "3. As an **adjacency list**: An array of $|\\mathcal{V}|$ unordered lists, where the $i$th list corresponds to the $i$th node, and contains all the nodes directly connected to node $i$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U6zwCe8b9i2"
      },
      "source": [
        "Example: Below is a directed graph with four nodes and five edges.\n",
        "\n",
        "\u003cimage src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/toy_graph.png\" width=\"400px\"\u003e\n",
        "\n",
        "The arrows on the edges indicate the direction of each edge, e.g. there is an edge going from node 0 to node 1. Between node 0 and node 3, there are two edges: one going from node 0 to node 3 and one from node 3 to node 0.\n",
        "\n",
        "Node 0 has out-degree of 2, since it has two outgoing edges, and an in-degree of 2, since it has two incoming edges.\n",
        "\n",
        "The list of edges is:\n",
        "$$[(0, 1), (0, 3), (1, 2), (2, 0), (3, 0)]$$\n",
        "\n",
        "As adjacency matrix:\n",
        "\n",
        "$$\\begin{array}{l|llll}\n",
        " source \\setminus dest    \u0026 n_0 \u0026 n_1 \u0026 n_2 \u0026 n_3 \\\\ \\hline\n",
        "n_0 \u0026 0    \u0026 1    \u0026 0    \u0026 1    \\\\\n",
        "n_1 \u0026 0    \u0026 0    \u0026 1    \u0026 0    \\\\\n",
        "n_2 \u0026 1    \u0026 0    \u0026 0    \u0026 0    \\\\\n",
        "n_3 \u0026 1    \u0026 0    \u0026 0    \u0026 0\n",
        "\\end{array}$$\n",
        "\n",
        "As adjacency list:\n",
        "\n",
        "$$[\\{1, 3\\}, \\{2\\}, \\{0\\}, \\{0\\}]$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spQGRxhPN8Eo"
      },
      "source": [
        "## Graph Prediction Tasks\n",
        "What are the kinds of problems we want to solve on graphs?\n",
        "\n",
        "\n",
        "The tasks fall into roughly three categories:\n",
        "\n",
        "1. **Node Classification**: E.g. what is the topic of a paper given a citation network of papers?\n",
        "2. **Link Prediction / Edge Classification**: E.g. are two people in a social network friends?\n",
        "3. **Graph Classification**: E.g. is this protein molecule (represented as a graph) likely going to be effective?\n",
        "\n",
        "\u003cimage src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/graph_tasks.png\" width=\"700px\"\u003e\n",
        "\n",
        "*The three main graph learning tasks. Image source: Petar Veliƒçkoviƒá.*\n",
        "\n",
        "Which examples of graph prediction tasks come to your mind? Which task types do they correspond to?\n",
        "\n",
        "We will create and train models on all three task types in this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C5YI9M0vwvb"
      },
      "source": [
        "## Intro to the jraph Library\n",
        "\n",
        "In the following sections, we will learn how to represent graphs and build GNNs in Python. We will use\n",
        "[jraph](https://github.com/deepmind/jraph), a lightweight library for working with GNNs in [JAX](https://github.com/google/jax).\n",
        "\n",
        "### Representing a graph in jraph\n",
        "\n",
        "In jraph, a graph is represented with a `GraphsTuple` object. In addition to defining the graph structure of nodes and edges, you can also store node features, edge features and global graph features in a `GraphsTuple`.\n",
        "\n",
        "In the `GraphsTuple`, edges are represented in two aligned arrays of node indices: senders (source nodes) and receivers (destinaton nodes).\n",
        "Each index corresponds to one edge, e.g. edge `i` goes from `senders[i]` to `receivers[i]`.\n",
        "\n",
        "You can even store multiple graphs in one `GraphsTuple` object.\n",
        "\n",
        "We will start with creating a simple directed graph with 4 nodes and 5 edges. We will also add toy features to the nodes, using `2*node_index` as the feature.\n",
        "\n",
        "We will later use this toy graph in the GCN demo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK0rGWf56-Uq"
      },
      "outputs": [],
      "source": [
        "def build_toy_graph() -\u003e jraph.GraphsTuple:\n",
        "  \"\"\"Define a four node graph, each node has a scalar as its feature.\"\"\"\n",
        "\n",
        "  # Nodes are defined implicitly by their features.\n",
        "  # We will add four nodes, each with a feature, e.g.\n",
        "  # node 0 has feature [0.],\n",
        "  # node 1 has featre [2.] etc.\n",
        "  # len(node_features) is the number of nodes.\n",
        "  node_features = jnp.array([[0.], [2.], [4.], [6.]])\n",
        "\n",
        "  # We will now specify 5 directed edges connecting the nodes we defined above.\n",
        "  # We define this with `senders` (source node indices) and `receivers`\n",
        "  # (destination node indices).\n",
        "  # For example, to add an edge from node 0 to node 1, we append 0 to senders,\n",
        "  # and 1 to receivers.\n",
        "  # We can do the same for all 5 edges:\n",
        "  # 0 -\u003e 1\n",
        "  # 1 -\u003e 2\n",
        "  # 2 -\u003e 0\n",
        "  # 3 -\u003e 0\n",
        "  # 0 -\u003e 3\n",
        "  senders = jnp.array([0, 1, 2, 3, 0])\n",
        "  receivers = jnp.array([1, 2, 0, 0, 3])\n",
        "\n",
        "  # You can optionally add edge attributes to the 5 edges.\n",
        "  edges = jnp.array([[5.], [6.], [7.], [8.], [8.]])\n",
        "\n",
        "  # We then save the number of nodes and the number of edges.\n",
        "  # This information is used to make running GNNs over multiple graphs\n",
        "  # in a GraphsTuple possible.\n",
        "  n_node = jnp.array([4])\n",
        "  n_edge = jnp.array([5])\n",
        "\n",
        "  # Optionally you can add `global` information, such as a graph label.\n",
        "  global_context = jnp.array([[1]]) # Same feature dims as nodes and edges.\n",
        "  graph = jraph.GraphsTuple(\n",
        "      nodes=node_features,\n",
        "      edges=edges,\n",
        "      senders=senders,\n",
        "      receivers=receivers,\n",
        "      n_node=n_node,\n",
        "      n_edge=n_edge,\n",
        "      globals=global_context\n",
        "      )\n",
        "  return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82Jtg_y-TqCW"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnWGPulK_tYE"
      },
      "source": [
        "#### Inspecting the GraphsTuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjfUYT_UzoQ9"
      },
      "outputs": [],
      "source": [
        "# Number of nodes\n",
        "# Note that `n_node` returns an array. The length of `n_node` corresponds to\n",
        "# the number of graphs stored in one `GraphsTuple`.\n",
        "# In this case, we only have one graph, so n_node has length 1.\n",
        "graph.n_node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQLF-mfOzXGK"
      },
      "outputs": [],
      "source": [
        "# Number of edges\n",
        "graph.n_edge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdzVSy04zp3-"
      },
      "outputs": [],
      "source": [
        "# Node features\n",
        "graph.nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9_VpQSZzua3"
      },
      "outputs": [],
      "source": [
        "# Edge features\n",
        "graph.edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvN0pbg0z8Ir"
      },
      "outputs": [],
      "source": [
        "# Edges\n",
        "graph.senders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNI4PVR-z-HL"
      },
      "outputs": [],
      "source": [
        "graph.receivers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayThRCYpz4wj"
      },
      "outputs": [],
      "source": [
        "# Graph-level features\n",
        "graph.globals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Pwh9e7d8gN"
      },
      "source": [
        "#### Visualizing the Graph\n",
        "To visualize the graph structure of the graph we created above, we will use the [`networkx`](networkx.org) library because it already has functions for drawing graphs.\n",
        "\n",
        "We first convert the `jraph.GraphsTuple` to a `networkx.DiGraph`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7q5ySSmVL3x"
      },
      "outputs": [],
      "source": [
        "def convert_jraph_to_networkx_graph(jraph_graph: jraph.GraphsTuple) -\u003e nx.Graph:\n",
        "  nodes, edges, receivers, senders, _, _, _ = jraph_graph\n",
        "  nx_graph = nx.DiGraph()\n",
        "  if nodes is None:\n",
        "    for n in range(jraph_graph.n_node[0]):\n",
        "      nx_graph.add_node(n)\n",
        "  else:\n",
        "    for n in range(jraph_graph.n_node[0]):\n",
        "      nx_graph.add_node(n, node_feature=nodes[n])\n",
        "  if edges is None:\n",
        "    for e in range(jraph_graph.n_edge[0]):\n",
        "      nx_graph.add_edge(int(senders[e]), int(receivers[e]))\n",
        "  else:\n",
        "    for e in range(jraph_graph.n_edge[0]):\n",
        "      nx_graph.add_edge(\n",
        "          int(senders[e]), int(receivers[e]), edge_feature=edges[e])\n",
        "  return nx_graph\n",
        "\n",
        "\n",
        "def draw_jraph_graph_structure(jraph_graph: jraph.GraphsTuple) -\u003e None:\n",
        "  nx_graph = convert_jraph_to_networkx_graph(jraph_graph)\n",
        "  pos = nx.spring_layout(nx_graph)\n",
        "  nx.draw(\n",
        "      nx_graph, pos=pos, with_labels=True, node_size=500, font_color='yellow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNK5SeajWQWO"
      },
      "outputs": [],
      "source": [
        "draw_jraph_graph_structure(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZRMF2d-h2pd"
      },
      "source": [
        "## Graph Convolutional Network (GCN) Layer\n",
        "Now let's implement our first graph network!\n",
        "\n",
        "The graph convolutional network, introduced by by Kipf et al. (2017) in https://arxiv.org/abs/1609.02907, is one of the basic graph network architectures. We will build its core building block, the graph convolutional layer.\n",
        "\n",
        "In a convolutional neural network (CNN), a convolutional filter (e.g. 3x3) is applied repeatedly to different parts of a larger input (e.g. 64x64) by striding across the input.\n",
        "\n",
        "In a GCN, a convolution filter is applied to the neighbourhoods around a node in a graph.\n",
        "\n",
        "However, there are also some differences to point out:\n",
        "In contrast to the CNN filter, the neighbourhoods in a GCN can be of different sizes, and there is no ordering of inputs. To see that, note that the CNN filter performs a weighted sum aggregation over the inputs with learnable weights, where each filter input has its own weight. In the GCN, the same weight is applied to all neighbours and the aggregation function is not learned. In other words, in a GCN, each neighbor contributes equally. This is why the CNN filter is not order-invariant, but the GCN filter is.\n",
        "\n",
        "\n",
        "\u003cimage src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/cnn_vs_gnn.png\" width=\"400px\"\u003e\n",
        "\n",
        "Comparison of CNN and GCN filters.\n",
        "Image source: https://arxiv.org/pdf/1901.00596.pdf\n",
        "\n",
        "More specifically, the GCN layer performs two steps:\n",
        "\n",
        "1. _Compute messages / update node features_: Create a feature vector $\\vec{h}_n$ for each node $n$ (e.g. with an MLP). This is going to be the message that this node will pass to neighboring nodes.\n",
        "2. _Message-passing / aggregate node features_: For each node, calculate a new feature vector $\\vec{h}'_n$ based on the messages (features) from the nodes in its neighborhood. In a directed graph, only nodes from incoming edges are counted as neighbors. The image below shows this aggregation step. There are multiple options for aggregation in a GCN, e.g. taking the mean, the sum, the min or max. (Later in this tutorial, we will also see how we can make the aggregation function dependent on the node features by adding an attention mechanism in the Graph Attention Network.)\n",
        "\n",
        "\u003cimage src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/graph_conv.png\" width=\"500px\"\u003e\n",
        "\n",
        "*\\\"A generic overview of a graph convolution operation, highlighting the relevant information for deriving the next-level features for every node in the graph.\\\"* Image source: Petar Veliƒçkoviƒá (https://github.com/PetarV-/TikZ)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc6fCm0zzOv-"
      },
      "source": [
        "### Simple GCN Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc7u-eHvg5Zp"
      },
      "outputs": [],
      "source": [
        "def apply_simplified_gcn(graph: jraph.GraphsTuple) -\u003e jraph.GraphsTuple:\n",
        "  # Unpack GraphsTuple\n",
        "  nodes, _, receivers, senders, _, _, _ = graph\n",
        "\n",
        "  # 1. Update node features\n",
        "  # For simplicity, we will first use an identify function here, and replace it\n",
        "  # with a trainable MLP block later.\n",
        "  update_node_fn = lambda nodes: nodes\n",
        "  nodes = update_node_fn(nodes)\n",
        "\n",
        "  # 2. Aggregate node features over nodes in neighborhood\n",
        "  # Equivalent to jnp.sum(n_node), but jittable\n",
        "  total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
        "  aggregate_nodes_fn = jax.ops.segment_sum\n",
        "\n",
        "  # Compute new node features by aggregating messages from neighboring nodes\n",
        "  nodes = tree.tree_map(lambda x: aggregate_nodes_fn(x[senders], receivers,\n",
        "                                        total_num_nodes), nodes)\n",
        "  out_graph = graph._replace(nodes=nodes)\n",
        "  return out_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs66eJ1ZawD2"
      },
      "source": [
        "We can now run the graph convolution on our toy graph from before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhG3r8P-aiBb"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRHJ_CYthY8P"
      },
      "source": [
        "Here is the visualized graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsaiSIZYbPzw"
      },
      "outputs": [],
      "source": [
        "draw_jraph_graph_structure(graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zAwAgy2bB1P"
      },
      "outputs": [],
      "source": [
        "out_graph = apply_simplified_gcn(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ryPhwtcGTx"
      },
      "source": [
        "Since we used the identity function for updating nodes and sum aggregation, we can verify the results pretty easily. As a reminder, in this toy graph, the node features are the same as the node index.\n",
        "\n",
        "Node 0: sum of features from node 2 and node 3 $\\rightarrow$ 10.\n",
        "\n",
        "Node 1: sum of features from node 0 $\\rightarrow$ 0.\n",
        "\n",
        "Node 2: sum of features from node 1 $\\rightarrow$ 2.\n",
        "\n",
        "Node 3: sum of features from node 0 $\\rightarrow$ 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwEtzddrbHri"
      },
      "outputs": [],
      "source": [
        "out_graph.nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rt3AB2tvv1o"
      },
      "source": [
        "### Add Trainable Parameters to GCN layer\n",
        "So far our graph convolution operation doesn't have any learnable parameters.\n",
        "Let's add an MLP block to the update function to make it trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK9W9J4GsDbC"
      },
      "outputs": [],
      "source": [
        "class MLP(hk.Module):\n",
        "  def __init__(self, features: jnp.ndarray):\n",
        "    super().__init__()\n",
        "    self.features = features\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray) -\u003e jnp.ndarray:\n",
        "    layers = []\n",
        "    for feat in self.features[:-1]:\n",
        "      layers.append(hk.Linear(feat))\n",
        "      layers.append(jax.nn.relu)\n",
        "    layers.append(hk.Linear(self.features[-1]))\n",
        "\n",
        "    mlp = hk.Sequential(layers)\n",
        "    return mlp(x)\n",
        "\n",
        "# Use MLP block to define the update node function\n",
        "update_node_fn = lambda x: MLP(features=[8, 4])(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQCaukgyOue"
      },
      "source": [
        "#### Check outputs of `update_node_fn` with MLP Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ImWUoi9s2-x"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BCg7cYwyBRw"
      },
      "outputs": [],
      "source": [
        "update_node_module = hk.without_apply_rng(hk.transform(update_node_fn))\n",
        "params = update_node_module.init(jax.random.PRNGKey(42), graph.nodes)\n",
        "out = update_node_module.apply(params, graph.nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooUu13gBzm2S"
      },
      "source": [
        "As output, we expect the updated node features. We should see one array of dim 4 for each of the 4 nodes, which is the result of applying a single MLP block to the features of each node individually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PblLCX67ubZ-"
      },
      "outputs": [],
      "source": [
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD1Qc7xK-SkI"
      },
      "source": [
        "#### Add Self-Edges (Edges connecting a node to itself)\n",
        "For each node, add an edge of the node onto itself. This way, nodes will include themselves in the aggregation step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8LO_UMN-ReR"
      },
      "outputs": [],
      "source": [
        "def add_self_edges_fn(receivers: jnp.ndarray, senders: jnp.ndarray,\n",
        "                      total_num_nodes: int) -\u003e Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "  \"\"\"Adds self edges. Assumes self edges are not in the graph yet.\"\"\"\n",
        "  receivers = jnp.concatenate((receivers, jnp.arange(total_num_nodes)), axis=0)\n",
        "  senders = jnp.concatenate((senders, jnp.arange(total_num_nodes)), axis=0)\n",
        "  return receivers, senders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RCGC0XbkjCV"
      },
      "source": [
        "#### Add Symmetric Normalization\n",
        "\n",
        "Note that the nodes may have different numbers of neighbors / degrees.\n",
        "This could lead to instabilities during neural network training, e.g. exploding or vanishing gradients. To address that, normalization is a commonly used method. In this case, we will normalize by node degrees.\n",
        "\n",
        "As a first attempt, we could count the number of incoming edges (including self-edge) and divide by that value.\n",
        "\n",
        "More formally, let $A$ be the adjacency matrix defining the edges of the graph.\n",
        "\n",
        "Then we define the degree matrix $D$ as a diagonal matrix with $D_{ii} = \\sum_jA_{ij}$ (the degree of node $i$)\n",
        "\n",
        "\n",
        "Now we can normalize $AH$ by dividing it by the node degrees:\n",
        "$${D}^{-1}AH$$\n",
        "\n",
        "To take both the in and out degrees into account, we can use symmetric normalization, which is also what Kipf and Welling proposed in their [paper](https://arxiv.org/abs/1609.02907):\n",
        "$$D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}H$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdxzqrpbj-jZ"
      },
      "source": [
        "### General GCN Layer\n",
        "Now we can write a more general and configurable version of the Graph Convolution layer, allowing the caller to specify:\n",
        "\n",
        "*   **`update_node_fn`**: Function to use to update node features (e.g. the MLP block version we just implemented)\n",
        "*   **`aggregate_nodes_fn`**: Aggregation function to use to aggregate messages from neighbourhood.\n",
        "*  **`add_self_edges`**: Whether to add self edges for aggregation step.\n",
        "* **`symmetric_normalization`**: Whether to add symmetric normalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkkOolOVh3nR"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L506\n",
        "def GraphConvolution(update_node_fn: Callable,\n",
        "                     aggregate_nodes_fn: Callable = jax.ops.segment_sum,\n",
        "                     add_self_edges: bool = False,\n",
        "                     symmetric_normalization: bool = True) -\u003e Callable:\n",
        "  \"\"\"Returns a method that applies a Graph Convolution layer.\n",
        "\n",
        "  Graph Convolutional layer as in https://arxiv.org/abs/1609.02907,\n",
        "  NOTE: This implementation does not add an activation after aggregation.\n",
        "  If you are stacking layers, you may want to add an activation between\n",
        "  each layer.\n",
        "  Args:\n",
        "    update_node_fn: function used to update the nodes. In the paper a single\n",
        "      layer MLP is used.\n",
        "    aggregate_nodes_fn: function used to aggregates the sender nodes.\n",
        "    add_self_edges: whether to add self edges to nodes in the graph as in the\n",
        "      paper definition of GCN. Defaults to False.\n",
        "    symmetric_normalization: whether to use symmetric normalization. Defaults to\n",
        "      True.\n",
        "\n",
        "  Returns:\n",
        "    A method that applies a Graph Convolution layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def _ApplyGCN(graph: jraph.GraphsTuple) -\u003e jraph.GraphsTuple:\n",
        "    \"\"\"Applies a Graph Convolution layer.\"\"\"\n",
        "    nodes, _, receivers, senders, _, _, _ = graph\n",
        "\n",
        "    # First pass nodes through the node updater.\n",
        "    nodes = update_node_fn(nodes)\n",
        "    # Equivalent to jnp.sum(n_node), but jittable\n",
        "    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
        "    if add_self_edges:\n",
        "      # We add self edges to the senders and receivers so that each node\n",
        "      # includes itself in aggregation.\n",
        "      # In principle, a `GraphsTuple` should partition by n_edge, but in\n",
        "      # this case it is not required since a GCN is agnostic to whether\n",
        "      # the `GraphsTuple` is a batch of graphs or a single large graph.\n",
        "      conv_receivers, conv_senders = add_self_edges_fn(receivers, senders,\n",
        "                                                       total_num_nodes)\n",
        "    else:\n",
        "      conv_senders = senders\n",
        "      conv_receivers = receivers\n",
        "\n",
        "    # pylint: disable=g-long-lambda\n",
        "    if symmetric_normalization:\n",
        "      # Calculate the normalization values.\n",
        "      count_edges = lambda x: jax.ops.segment_sum(\n",
        "          jnp.ones_like(conv_senders), x, total_num_nodes)\n",
        "      sender_degree = count_edges(conv_senders)\n",
        "      receiver_degree = count_edges(conv_receivers)\n",
        "\n",
        "      # Pre normalize by sqrt sender degree.\n",
        "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
        "      nodes = tree.tree_map(\n",
        "          lambda x: x * jax.lax.rsqrt(jnp.maximum(sender_degree, 1.0))[:, None],\n",
        "          nodes,\n",
        "      )\n",
        "      # Aggregate the pre-normalized nodes.\n",
        "      nodes = tree.tree_map(\n",
        "          lambda x: aggregate_nodes_fn(x[conv_senders], conv_receivers,\n",
        "                                       total_num_nodes), nodes)\n",
        "      # Post normalize by sqrt receiver degree.\n",
        "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
        "      nodes = tree.tree_map(\n",
        "          lambda x:\n",
        "          (x * jax.lax.rsqrt(jnp.maximum(receiver_degree, 1.0))[:, None]),\n",
        "          nodes,\n",
        "      )\n",
        "    else:\n",
        "      nodes = tree.tree_map(\n",
        "          lambda x: aggregate_nodes_fn(x[conv_senders], conv_receivers,\n",
        "                                       total_num_nodes), nodes)\n",
        "    # pylint: enable=g-long-lambda\n",
        "    return graph._replace(nodes=nodes)\n",
        "\n",
        "  return _ApplyGCN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKxcA6XAza33"
      },
      "source": [
        "#### Test General GCN Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TgyCkE9KlUG"
      },
      "outputs": [],
      "source": [
        "gcn_layer = GraphConvolution(\n",
        "    update_node_fn=lambda n: MLP(features=[8, 4])(n),\n",
        "    aggregate_nodes_fn=jax.ops.segment_sum,\n",
        "    add_self_edges=True,\n",
        "    symmetric_normalization=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gTPEWvU0DDB"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()\n",
        "network = hk.without_apply_rng(hk.transform(gcn_layer))\n",
        "params = network.init(jax.random.PRNGKey(42), graph)\n",
        "out_graph = network.apply(params, graph)\n",
        "out_graph.nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lha8rbQ78l3S"
      },
      "source": [
        "### Build GCN Model with Multiple Layers\n",
        "With a single GCN layer, a node's representation after the GCN layer is only\n",
        "influenced by its direct neighbourhood. However, we may want to consider larger neighbourhoods, i.e. more than just 1 hop away. To achieve that, we can stack\n",
        "multiple GCN layers, similar to how stacking CNN layers expands the input region.\n",
        "\n",
        "We will define a network with three GCN layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYOQBh848k2U"
      },
      "outputs": [],
      "source": [
        "def gcn(graph: jraph.GraphsTuple) -\u003e jraph.GraphsTuple:\n",
        "  \"\"\"Defines a graph neural network with 3 GCN layers.\n",
        "  Args:\n",
        "    graph: GraphsTuple the network processes.\n",
        "\n",
        "  Returns:\n",
        "    output graph with updated node values.\n",
        "  \"\"\"\n",
        "  gn = GraphConvolution(\n",
        "      update_node_fn=lambda n: jax.nn.relu(hk.Linear(8)(n)),\n",
        "      add_self_edges=True)\n",
        "  graph = gn(graph)\n",
        "\n",
        "  gn = GraphConvolution(\n",
        "      update_node_fn=lambda n: jax.nn.relu(hk.Linear(4)(n)),\n",
        "      add_self_edges=True)\n",
        "  graph = gn(graph)\n",
        "\n",
        "  gn = GraphConvolution(\n",
        "      update_node_fn=hk.Linear(2))\n",
        "  graph = gn(graph)\n",
        "  return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6CisEhz_A-a"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()\n",
        "network = hk.without_apply_rng(hk.transform(gcn))\n",
        "params = network.init(jax.random.PRNGKey(42), graph)\n",
        "out_graph = network.apply(params, graph)\n",
        "out_graph.nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5t7kw7SE_h4"
      },
      "source": [
        "### Node Classification with GCN on Karate Club Dataset\n",
        "\n",
        "Time to try out our GCN on our first graph prediction task!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ_w2kkWoAq4"
      },
      "source": [
        "#### Zachary's Karate Club Dataset\n",
        "\n",
        "[Zachary's karate club](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) is a small dataset commonly used as an example for a social graph. It's great for demo purposes, as it's easy to visualize and quick to train a model on it.\n",
        "\n",
        "A node represents a student or instructor in the club. An edge means that those two people have interacted outside of the class. There are two instructors in the club.\n",
        "\n",
        "Each student is assigned to one of two instructors.\n",
        "\n",
        "#### Optimizing the GCN on the Karate Club Node Classification Task\n",
        "\n",
        "The task is to predict the assignment of students to instructors, given the social graph and only knowing the assignment of two nodes (the two instructors) a priori.\n",
        "\n",
        "In other words, out of the 34 nodes, only two nodes are labeled, and we are trying to optimize the assignment of the other 32 nodes, by **maximizing the log-likelihood of the two known node assignments**.\n",
        "\n",
        "We will compute the accuracy of our node assignments by comparing to the ground-truth assignments. **Note that the ground-truth for the 32 student nodes is not used in the loss function itself.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SE5DQoXWQJR"
      },
      "source": [
        "Let's load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YGQXeGN_E3J"
      },
      "outputs": [],
      "source": [
        "\"\"\"Zachary's karate club example.\n",
        "From https://github.com/deepmind/jraph/blob/master/jraph/examples/zacharys_karate_club.py.\n",
        "Here we train a graph neural network to process Zachary's karate club.\n",
        "https://en.wikipedia.org/wiki/Zachary%27s_karate_club\n",
        "Zachary's karate club is used in the literature as an example of a social graph.\n",
        "Here we use a graphnet to optimize the assignments of the students in the\n",
        "karate club to two distinct karate instructors (Mr. Hi and John A).\n",
        "\"\"\"\n",
        "\n",
        "def get_zacharys_karate_club() -\u003e jraph.GraphsTuple:\n",
        "  \"\"\"Returns GraphsTuple representing Zachary's karate club.\"\"\"\n",
        "  social_graph = [\n",
        "      (1, 0), (2, 0), (2, 1), (3, 0), (3, 1), (3, 2),\n",
        "      (4, 0), (5, 0), (6, 0), (6, 4), (6, 5), (7, 0), (7, 1),\n",
        "      (7, 2), (7, 3), (8, 0), (8, 2), (9, 2), (10, 0), (10, 4),\n",
        "      (10, 5), (11, 0), (12, 0), (12, 3), (13, 0), (13, 1), (13, 2),\n",
        "      (13, 3), (16, 5), (16, 6), (17, 0), (17, 1), (19, 0), (19, 1),\n",
        "      (21, 0), (21, 1), (25, 23), (25, 24), (27, 2), (27, 23),\n",
        "      (27, 24), (28, 2), (29, 23), (29, 26), (30, 1), (30, 8),\n",
        "      (31, 0), (31, 24), (31, 25), (31, 28), (32, 2), (32, 8),\n",
        "      (32, 14), (32, 15), (32, 18), (32, 20), (32, 22), (32, 23),\n",
        "      (32, 29), (32, 30), (32, 31), (33, 8), (33, 9), (33, 13),\n",
        "      (33, 14), (33, 15), (33, 18), (33, 19), (33, 20), (33, 22),\n",
        "      (33, 23), (33, 26), (33, 27), (33, 28), (33, 29), (33, 30),\n",
        "      (33, 31), (33, 32)]\n",
        "  # Add reverse edges.\n",
        "  social_graph += [(edge[1], edge[0]) for edge in social_graph]\n",
        "  n_club_members = 34\n",
        "\n",
        "  return jraph.GraphsTuple(\n",
        "      n_node=jnp.asarray([n_club_members]),\n",
        "      n_edge=jnp.asarray([len(social_graph)]),\n",
        "      # One-hot encoding for nodes, i.e. argmax(nodes) = node index.\n",
        "      nodes=jnp.eye(n_club_members),\n",
        "      # No edge features.\n",
        "      edges=None,\n",
        "      globals=None,\n",
        "      senders=jnp.asarray([edge[0] for edge in social_graph]),\n",
        "      receivers=jnp.asarray([edge[1] for edge in social_graph]))\n",
        "\n",
        "def get_ground_truth_assignments_for_zacharys_karate_club() -\u003e jnp.ndarray:\n",
        "  \"\"\"Returns ground truth assignments for Zachary's karate club.\"\"\"\n",
        "  return jnp.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
        "                    0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyJk-Mq7EKoU"
      },
      "outputs": [],
      "source": [
        "graph = get_zacharys_karate_club()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4v5LnKgEN8l"
      },
      "outputs": [],
      "source": [
        "print(f'Number of nodes: {graph.n_node[0]}')\n",
        "print(f'Number of edges: {graph.n_edge[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi2DQYzjngR8"
      },
      "source": [
        "Visualize the karate club graph with circular node layout:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MVtGGJwg_B8"
      },
      "outputs": [],
      "source": [
        "nx_graph = convert_jraph_to_networkx_graph(graph)\n",
        "pos = nx.circular_layout(nx_graph)\n",
        "plt.figure(figsize=(6, 6))\n",
        "nx.draw(nx_graph, pos=pos, with_labels = True, node_size=500, font_color='yellow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afMhKYScQvMp"
      },
      "source": [
        "Define the GCN with the `GraphConvolution` layers we implemented:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAMlrQnIc7iO"
      },
      "outputs": [],
      "source": [
        "def gcn_definition(graph: jraph.GraphsTuple) -\u003e jraph.GraphsTuple:\n",
        "  \"\"\"Defines a GCN for the karate club task.\n",
        "  Args:\n",
        "    graph: GraphsTuple the network processes.\n",
        "\n",
        "  Returns:\n",
        "    output graph with updated node values.\n",
        "  \"\"\"\n",
        "  gn = GraphConvolution(\n",
        "      update_node_fn=lambda n: jax.nn.relu(hk.Linear(8)(n)),\n",
        "      add_self_edges=True)\n",
        "  graph = gn(graph)\n",
        "\n",
        "  gn = GraphConvolution(\n",
        "      update_node_fn=hk.Linear(2)) # output dim is 2 because we have 2 output classes.\n",
        "  graph = gn(graph)\n",
        "  return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCZI26T9Q2vy"
      },
      "source": [
        "Training and evaluation code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n1kCuqtkvfm"
      },
      "outputs": [],
      "source": [
        "def optimize_club(network: hk.Transformed, num_steps: int) -\u003e jnp.ndarray:\n",
        "  \"\"\"Solves the karate club problem by optimizing the assignments of students.\"\"\"\n",
        "  zacharys_karate_club = get_zacharys_karate_club()\n",
        "  labels = get_ground_truth_assignments_for_zacharys_karate_club()\n",
        "  params = network.init(jax.random.PRNGKey(42), zacharys_karate_club)\n",
        "\n",
        "  @jax.jit\n",
        "  def predict(params: hk.Params) -\u003e jnp.ndarray:\n",
        "    decoded_graph = network.apply(params, zacharys_karate_club)\n",
        "    return jnp.argmax(decoded_graph.nodes, axis=1)\n",
        "\n",
        "  @jax.jit\n",
        "  def prediction_loss(params: hk.Params) -\u003e jnp.ndarray:\n",
        "    decoded_graph = network.apply(params, zacharys_karate_club)\n",
        "    # We interpret the decoded nodes as a pair of logits for each node.\n",
        "    log_prob = jax.nn.log_softmax(decoded_graph.nodes)\n",
        "    # The only two assignments we know a-priori are those of Mr. Hi (Node 0)\n",
        "    # and John A (Node 33).\n",
        "    return -(log_prob[0, 0] + log_prob[33, 1])\n",
        "\n",
        "  opt_init, opt_update = optax.adam(1e-2)\n",
        "  opt_state = opt_init(params)\n",
        "\n",
        "  @jax.jit\n",
        "  def update(params: hk.Params, opt_state) -\u003e Tuple[hk.Params, Any]:\n",
        "    \"\"\"Returns updated params and state.\"\"\"\n",
        "    g = jax.grad(prediction_loss)(params)\n",
        "    updates, opt_state = opt_update(g, opt_state)\n",
        "    return optax.apply_updates(params, updates), opt_state\n",
        "\n",
        "  @jax.jit\n",
        "  def accuracy(params: hk.Params) -\u003e jnp.ndarray:\n",
        "    decoded_graph = network.apply(params, zacharys_karate_club)\n",
        "    return jnp.mean(jnp.argmax(decoded_graph.nodes, axis=1) == labels)\n",
        "\n",
        "  for step in range(num_steps):\n",
        "    print(f\"step {step} accuracy {accuracy(params).item():.2f}\")\n",
        "    params, opt_state = update(params, opt_state)\n",
        "\n",
        "  return predict(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkyao6e5RBug"
      },
      "source": [
        "Let's train the GCN! We expect this model reach an accuracy of about 0.91."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcp00eY46vTE"
      },
      "outputs": [],
      "source": [
        "network = hk.without_apply_rng(hk.transform(gcn_definition))\n",
        "result = optimize_club(network, num_steps=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rUwloZhTFW7"
      },
      "source": [
        "\n",
        "Try modifying the model parameters to see if you can improve the accuracy!\n",
        "\n",
        "You can also modify the dataset itself, and see how that influences model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3rPI44_VbaY"
      },
      "source": [
        "Node assignments predicted by the model at the end of training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBKh7f-SPBnX"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0gNu67xP-_K"
      },
      "source": [
        "Visualize ground truth and predicted node assignments:\n",
        "\n",
        "What do you think of the results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndklBMr6PHLY"
      },
      "outputs": [],
      "source": [
        "zacharys_karate_club = get_zacharys_karate_club()\n",
        "nx_graph = convert_jraph_to_networkx_graph(zacharys_karate_club)\n",
        "pos = nx.circular_layout(nx_graph)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 7))\n",
        "ax1 = fig.add_subplot(121)\n",
        "nx.draw(\n",
        "    nx_graph,\n",
        "    pos=pos,\n",
        "    with_labels=True,\n",
        "    node_size=500,\n",
        "    node_color=result.tolist(),\n",
        "    font_color='white')\n",
        "ax1.title.set_text('Predicted Node Assignments with GCN')\n",
        "\n",
        "gt_labels = get_ground_truth_assignments_for_zacharys_karate_club()\n",
        "ax2 = fig.add_subplot(122)\n",
        "nx.draw(\n",
        "    nx_graph,\n",
        "    pos=pos,\n",
        "    with_labels=True,\n",
        "    node_size=500,\n",
        "    node_color=gt_labels.tolist(),\n",
        "    font_color='white')\n",
        "ax2.title.set_text('Ground-Truth Node Assignments')\n",
        "fig.suptitle('Do you spot the difference? üòê', y=-0.01)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg8g96NdBCK6"
      },
      "source": [
        "## Graph Attention (GAT) Layer\n",
        "\n",
        "While the GCN we covered in the previous section can learn meaningful representations, it also has some shortcomings. Can you think of any?\n",
        "\n",
        "In the GCN layer, the messages from all its neighbours and the node itself are equally weighted. This may lead to loss of node-specific information. E.g., consider the case when a set of nodes shares the same set of neighbors, and start out with different node features. Then because of averaging, their resulting output features would be the same. Adding self-edges mitigates this issue by a small amount, but this problem is magnified with increasing number of GCN layers and number of edges connecting to a node.\n",
        "\n",
        "The graph attention (GAT) mechanism, as proposed by [Velickovic et al. ( 2017)](https://arxiv.org/abs/1710.10903), allows the network to learn how to weigh / assign importance to the node features from the neighbourhood when computing the new node features. This is very similar to the idea of using attention in Transformers, which were introduced in [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "(One could even argue that Transformers are graph attention networks operating on the special case of fully-connected graphs.)\n",
        "\n",
        "In the figure below, $\\vec{h}$ are the node features and $\\vec{\\alpha}$ are the learned attention weights.\n",
        "\n",
        "\n",
        "\n",
        "\u003cimage src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/gat1.png\" width=\"400px\"\u003e\n",
        "\n",
        "Figure Credit: [Velickovic et al. ( 2017)](https://arxiv.org/abs/1710.10903).\n",
        "(Detail: This image is showing multi-headed attention with 3 heads, each color corresponding to a different head. At the end, an aggregation function is applied over all the heads.)\n",
        "\n",
        "To obtain the output node features of the GAT layer, we compute:\n",
        "\n",
        "$$ \\vec{h}'_i = \\sum _{j \\in \\mathcal{N}(i)}\\alpha_{ij} \\mathbf{W} \\vec{h}_j$$\n",
        "Here, $\\mathbf{W}$ is a weight matrix which performs a linear transformation on the input.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qa6dOLBGLlP"
      },
      "source": [
        "\n",
        "How do we obtain $\\alpha$, or in other words, learn what to pay attention to?\n",
        "\n",
        "Intuitively, the attention coefficient $\\alpha_{ij}$ should rely on both the transformed features from nodes $i$ and $j$. So let's first define an attention mechanism function $\\mathrm{attention\\_fn}$ that computes the intermediary attention coefficients $e_{ij}$:\n",
        "$$ e_{ij} = \\mathrm{attention\\_fn}(\\mathbf{W}\\vec{h}_i, \\mathbf{W}\\vec{h}_j)$$\n",
        "\n",
        "To obtain normalized attention weights $\\alpha$, we apply a softmax:\n",
        "$$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum _{j \\in \\mathcal{N}(i)}\\exp(e_{ij})}$$\n",
        "\n",
        "For the function $a$, the authors of the GAT paper chose to concatenate the transformed node features (denoted by $||$) and apply a single-layer feedforward network, parameterized by a weight vector $\\vec{\\mathbf{a}}$ and with LeakyRelu as non-linearity.\n",
        "\n",
        "In the implementation below, we refer to $\\mathbf{W}$ as `attention_query_fn` and $att\\_fn$ as `attention_logit_fn`.\n",
        "\n",
        "$$\\mathrm{attention\\_fn}(\\mathbf{W}\\vec{h}_i, \\mathbf{W}\\vec{h}_j) =  \\text{LeakyReLU}(\\vec{\\mathbf{a}}(\\mathbf{W}\\vec{h}_i || \\mathbf{W}\\vec{h}_j))$$\n",
        "\n",
        "The figure below summarizes this attention mechanism visually.\n",
        "\n",
        "\u003cimage src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/gat2.png\" width=\"300px\"\u003e\n",
        "\n",
        "Figure Credit: Petar Velickovic.\n",
        "\u003c!-- $\\sum_{j \\in \\mathcal{N}(i)}\\vec{\\alpha}_{ij} \\stackrel{!}{=}\n",
        "1 $  --\u003e\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJqT6mjgBBmQ"
      },
      "outputs": [],
      "source": [
        "# GAT implementation adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L442.\n",
        "def GAT(attention_query_fn: Callable,\n",
        "        attention_logit_fn: Callable,\n",
        "        node_update_fn: Optional[Callable] = None,\n",
        "        add_self_edges: bool = True) -\u003e Callable:\n",
        "  \"\"\"Returns a method that applies a Graph Attention Network layer.\n",
        "\n",
        "  Graph Attention message passing as described in\n",
        "  https://arxiv.org/pdf/1710.10903.pdf. This model expects node features as a\n",
        "  jnp.array, may use edge features for computing attention weights, and\n",
        "  ignore global features. It does not support nests.\n",
        "  Args:\n",
        "    attention_query_fn: function that generates attention queries from sender\n",
        "      node features.\n",
        "    attention_logit_fn: function that converts attention queries into logits for\n",
        "      softmax attention.\n",
        "    node_update_fn: function that updates the aggregated messages. If None, will\n",
        "      apply leaky relu and concatenate (if using multi-head attention).\n",
        "\n",
        "  Returns:\n",
        "    A function that applies a Graph Attention layer.\n",
        "  \"\"\"\n",
        "  # pylint: disable=g-long-lambda\n",
        "  if node_update_fn is None:\n",
        "    # By default, apply the leaky relu and then concatenate the heads on the\n",
        "    # feature axis.\n",
        "    node_update_fn = lambda x: jnp.reshape(\n",
        "        jax.nn.leaky_relu(x), (x.shape[0], -1))\n",
        "\n",
        "  def _ApplyGAT(graph: jraph.GraphsTuple) -\u003e jraph.GraphsTuple:\n",
        "    \"\"\"Applies a Graph Attention layer.\"\"\"\n",
        "    nodes, edges, receivers, senders, _, _, _ = graph\n",
        "    # Equivalent to the sum of n_node, but statically known.\n",
        "    try:\n",
        "      sum_n_node = nodes.shape[0]\n",
        "    except IndexError:\n",
        "      raise IndexError('GAT requires node features')\n",
        "\n",
        "    # Pass nodes through the attention query function to transform\n",
        "    # node features, e.g. with an MLP.\n",
        "    nodes = attention_query_fn(nodes)\n",
        "\n",
        "    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
        "    if add_self_edges:\n",
        "      # We add self edges to the senders and receivers so that each node\n",
        "      # includes itself in aggregation.\n",
        "      receivers, senders = add_self_edges_fn(receivers, senders,\n",
        "                                             total_num_nodes)\n",
        "\n",
        "    # We compute the softmax logits using a function that takes the\n",
        "    # embedded sender and receiver attributes.\n",
        "    sent_attributes = nodes[senders]\n",
        "    received_attributes = nodes[receivers]\n",
        "    att_softmax_logits = attention_logit_fn(sent_attributes,\n",
        "                                            received_attributes, edges)\n",
        "\n",
        "    # Compute the attention softmax weights on the entire tree.\n",
        "    att_weights = jraph.segment_softmax(\n",
        "        att_softmax_logits, segment_ids=receivers, num_segments=sum_n_node)\n",
        "\n",
        "    # Apply attention weights.\n",
        "    messages = sent_attributes * att_weights\n",
        "    # Aggregate messages to nodes.\n",
        "    nodes = jax.ops.segment_sum(messages, receivers, num_segments=sum_n_node)\n",
        "\n",
        "    # Apply an update function to the aggregated messages.\n",
        "    nodes = node_update_fn(nodes)\n",
        "\n",
        "    return graph._replace(nodes=nodes)\n",
        "\n",
        "  # pylint: enable=g-long-lambda\n",
        "  return _ApplyGAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t9bNfSA9-HR"
      },
      "source": [
        "#### Test GAT Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjpYLFvv9-HT"
      },
      "outputs": [],
      "source": [
        "def attention_logit_fn(sender_attr: jnp.ndarray, receiver_attr: jnp.ndarray,\n",
        "                       edges: jnp.ndarray) -\u003e jnp.ndarray:\n",
        "  del edges\n",
        "  x = jnp.concatenate((sender_attr, receiver_attr), axis=1)\n",
        "  return hk.Linear(1)(x)\n",
        "\n",
        "\n",
        "gat_layer = GAT(\n",
        "    attention_query_fn=lambda n: hk.Linear(8)\n",
        "    (n),  # Applies W to the node features\n",
        "    attention_logit_fn=attention_logit_fn,\n",
        "    node_update_fn=None,\n",
        "    add_self_edges=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV5PndNz9-HT"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()\n",
        "network = hk.without_apply_rng(hk.transform(gat_layer))\n",
        "params = network.init(jax.random.PRNGKey(42), graph)\n",
        "out_graph = network.apply(params, graph)\n",
        "out_graph.nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anfVGJwBe27v"
      },
      "source": [
        "### Train GAT Model on Karate Club Dataset\n",
        "We will now repeat the karate club experiment with a GAT network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUXJcRnVMr4X"
      },
      "outputs": [],
      "source": [
        "def gat_definition(graph: jraph.GraphsTuple) -\u003e jraph.GraphsTuple:\n",
        "  \"\"\"Defines a GAT network for the karate club node classification task.\n",
        "\n",
        "  Args:\n",
        "    graph: GraphsTuple the network processes.\n",
        "\n",
        "  Returns:\n",
        "    output graph with updated node values.\n",
        "  \"\"\"\n",
        "\n",
        "  def _attention_logit_fn(sender_attr: jnp.ndarray, receiver_attr: jnp.ndarray,\n",
        "                          edges: jnp.ndarray) -\u003e jnp.ndarray:\n",
        "    del edges\n",
        "    x = jnp.concatenate((sender_attr, receiver_attr), axis=1)\n",
        "    return hk.Linear(1)(x)\n",
        "\n",
        "  gn = GAT(\n",
        "      attention_query_fn=lambda n: hk.Linear(8)(n),\n",
        "      attention_logit_fn=_attention_logit_fn,\n",
        "      node_update_fn=None,\n",
        "      add_self_edges=True)\n",
        "  graph = gn(graph)\n",
        "\n",
        "  gn = GAT(\n",
        "      attention_query_fn=lambda n: hk.Linear(8)(n),\n",
        "      attention_logit_fn=_attention_logit_fn,\n",
        "      node_update_fn=hk.Linear(2),\n",
        "      add_self_edges=True)\n",
        "  graph = gn(graph)\n",
        "  return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51AwRipThKrH"
      },
      "source": [
        "Let's train the model!\n",
        "\n",
        "We expect the model to reach an accuracy of about 0.97."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEHco8n-Mr4b"
      },
      "outputs": [],
      "source": [
        "network = hk.without_apply_rng(hk.transform(gat_definition))\n",
        "result = optimize_club(network, num_steps=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Cns9EITp0k"
      },
      "source": [
        "The final node assignment predicted by the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QWqU8AfdkkV"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0leRi5igUUyd"
      },
      "outputs": [],
      "source": [
        "zacharys_karate_club = get_zacharys_karate_club()\n",
        "nx_graph = convert_jraph_to_networkx_graph(zacharys_karate_club)\n",
        "pos = nx.circular_layout(nx_graph)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 7))\n",
        "ax1 = fig.add_subplot(121)\n",
        "nx.draw(\n",
        "    nx_graph,\n",
        "    pos=pos,\n",
        "    with_labels=True,\n",
        "    node_size=500,\n",
        "    node_color=result.tolist(),\n",
        "    font_color='white')\n",
        "ax1.title.set_text('Predicted Node Assignments with GAT')\n",
        "\n",
        "gt_labels = get_ground_truth_assignments_for_zacharys_karate_club()\n",
        "ax2 = fig.add_subplot(122)\n",
        "nx.draw(\n",
        "    nx_graph,\n",
        "    pos=pos,\n",
        "    with_labels=True,\n",
        "    node_size=500,\n",
        "    node_color=gt_labels.tolist(),\n",
        "    font_color='white')\n",
        "ax2.title.set_text('Ground-Truth Node Assignments')\n",
        "fig.suptitle('Do you spot the difference? üòê', y=-0.01)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5TxaTGzBkBa"
      },
      "source": [
        "## Graph Classification on MUTAG (Molecules)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPiCe4-UQ0fq"
      },
      "source": [
        "In the previous section, we used our GCN and GAT networks on a node classification problem. Now, let's use the same model architectures on a **graph classification task**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BllPGaGndBg"
      },
      "source": [
        "The main difference from our previous setup is that instead of observing individual node latents, we are now attempting to summarize them into one embedding vector, representative of the entire graph, which we then use to predict the class of this graph.\n",
        "\n",
        "We will do this on one of the most common tasks of this type -- **molecular property prediction**, where molecules are represented as graphs. Nodes correspond to atoms, and edges represent the bonds between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inDdiyKrqZUI"
      },
      "source": [
        "We will use the **MUTAG** dataset for this example, a common dataset from the [TUDatasets](https://chrsmrrs.github.io/datasets/) collection.\n",
        "\n",
        "We have converted this dataset to be compatible with jraph and will download it in the cell below.\n",
        "\n",
        "Citation for TUDatasets: [Morris, Christopher, et al. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint arXiv:2007.08663. 2020.](https://chrsmrrs.github.io/datasets/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO_SOVcmOH_q"
      },
      "outputs": [],
      "source": [
        "# Download jraph version of MUTAG.\n",
        "!wget -P /tmp/ https://storage.googleapis.com/dm-educational/assets/graph-nets/jraph_datasets/mutag.pickle\n",
        "with open('/tmp/mutag.pickle', 'rb') as f:\n",
        "  mutag_ds = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs32Jv8chdLM"
      },
      "source": [
        "The dataset is saved as a list of examples, each example is a dictionary containing an input_graph and its corresponding target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1oCfEIoJypp"
      },
      "outputs": [],
      "source": [
        "len(mutag_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVLUk5LAKBgT"
      },
      "outputs": [],
      "source": [
        "# Inspect the first graph\n",
        "g = mutag_ds[0]['input_graph']\n",
        "print(f'Number of nodes: {g.n_node[0]}')\n",
        "print(f'Number of edges: {g.n_edge[0]}')\n",
        "print(f'Node features shape: {g.nodes.shape}')\n",
        "print(f'Edge features shape: {g.edges.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ62jkKuyWRk"
      },
      "outputs": [],
      "source": [
        "draw_jraph_graph_structure(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4_uZqBxKjtz"
      },
      "outputs": [],
      "source": [
        "# Target for first graph\n",
        "print(f\"Target: {mutag_ds[0]['target']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yVF5gt8oz2N"
      },
      "source": [
        "We see that there are 188 graphs, to be classified in one of 2 classes, representing \"their mutagenic effect on a specific gram negative bacterium\". Node features represent the 1-hot encoding of the atom type (0=C, 1=N, 2=O, 3=F, 4=I, 5=Cl, 6=Br). Edge features (`edge_attr`) represent the bond type, which we will here ignore.\n",
        "\n",
        "Let's split the dataset to use the first 150 graphs as the training set (and the rest as the test set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwWngXmKtWa5"
      },
      "outputs": [],
      "source": [
        "train_mutag_ds = mutag_ds[:150]\n",
        "test_mutag_ds = mutag_ds[150:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOIDRiUq8nNK"
      },
      "source": [
        "#### Padding Graphs to Speed Up Training\n",
        "\n",
        "Since jax recompiles the program for each graph size, training would take a long time due to recompilation for different graph sizes. To address that, we pad the number of nodes and edges in the graphs to nearest power of two. Since jax maintains a cache\n",
        "of compiled programs, the compilation cost is amortized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGhnsIovZQpo"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/ogb_examples/train.py\n",
        "def _nearest_bigger_power_of_two(x: int) -\u003e int:\n",
        "  \"\"\"Computes the nearest power of two greater than x for padding.\"\"\"\n",
        "  y = 2\n",
        "  while y \u003c x:\n",
        "    y *= 2\n",
        "  return y\n",
        "\n",
        "def pad_graph_to_nearest_power_of_two(\n",
        "    graphs_tuple: jraph.GraphsTuple) -\u003e jraph.GraphsTuple:\n",
        "  \"\"\"Pads a batched `GraphsTuple` to the nearest power of two.\n",
        "  For example, if a `GraphsTuple` has 7 nodes, 5 edges and 3 graphs, this method\n",
        "  would pad the `GraphsTuple` nodes and edges:\n",
        "    7 nodes --\u003e 8 nodes (2^3)\n",
        "    5 edges --\u003e 8 edges (2^3)\n",
        "  And since padding is accomplished using `jraph.pad_with_graphs`, an extra\n",
        "  graph and node is added:\n",
        "    8 nodes --\u003e 9 nodes\n",
        "    3 graphs --\u003e 4 graphs\n",
        "  Args:\n",
        "    graphs_tuple: a batched `GraphsTuple` (can be batch size 1).\n",
        "  Returns:\n",
        "    A graphs_tuple batched to the nearest power of two.\n",
        "  \"\"\"\n",
        "  # Add 1 since we need at least one padding node for pad_with_graphs.\n",
        "  pad_nodes_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_node)) + 1\n",
        "  pad_edges_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_edge))\n",
        "  # Add 1 since we need at least one padding graph for pad_with_graphs.\n",
        "  # We do not pad to nearest power of two because the batch size is fixed.\n",
        "  pad_graphs_to = graphs_tuple.n_node.shape[0] + 1\n",
        "  return jraph.pad_with_graphs(graphs_tuple, pad_nodes_to, pad_edges_to,\n",
        "                               pad_graphs_to)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np1NBMZW9YOp"
      },
      "source": [
        "#### Graph Network Model Definition\n",
        "\n",
        "We will use `jraph.GraphNetwork()` to build our graph model. The `GraphNetwork` architecture is defined in [Battaglia et al. (2018)](https://arxiv.org/pdf/1806.01261.pdf).\n",
        "\n",
        "We first define update functions for nodes, edges, and the full graph (global). We will use MLP blocks for all three."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkn2WgDsarq7"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/ogb_examples/train.py\n",
        "@jraph.concatenated_args\n",
        "def edge_update_fn(feats: jnp.ndarray) -\u003e jnp.ndarray:\n",
        "  \"\"\"Edge update function for graph net.\"\"\"\n",
        "  net = hk.Sequential(\n",
        "      [hk.Linear(128), jax.nn.relu,\n",
        "       hk.Linear(128)])\n",
        "  return net(feats)\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def node_update_fn(feats: jnp.ndarray) -\u003e jnp.ndarray:\n",
        "  \"\"\"Node update function for graph net.\"\"\"\n",
        "  net = hk.Sequential(\n",
        "      [hk.Linear(128), jax.nn.relu,\n",
        "       hk.Linear(128)])\n",
        "  return net(feats)\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def update_global_fn(feats: jnp.ndarray) -\u003e jnp.ndarray:\n",
        "  \"\"\"Global update function for graph net.\"\"\"\n",
        "  # MUTAG is a binary classification task, so output pos neg logits.\n",
        "  net = hk.Sequential(\n",
        "      [hk.Linear(128), jax.nn.relu,\n",
        "       hk.Linear(2)])\n",
        "  return net(feats)\n",
        "\n",
        "def net_fn(graph: jraph.GraphsTuple) -\u003e jraph.GraphsTuple:\n",
        "  # Add a global paramater for graph classification.\n",
        "  graph = graph._replace(globals=jnp.zeros([graph.n_node.shape[0], 1]))\n",
        "  embedder = jraph.GraphMapFeatures(\n",
        "      hk.Linear(128), hk.Linear(128), hk.Linear(128))\n",
        "  net = jraph.GraphNetwork(\n",
        "      update_node_fn=node_update_fn,\n",
        "      update_edge_fn=edge_update_fn,\n",
        "      update_global_fn=update_global_fn)\n",
        "  return net(embedder(graph))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY6qHvGkbKqS"
      },
      "source": [
        "#### Loss and Accuracy Function\n",
        "Define the classification cross-entropy loss and accuracy function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vEmAsr5bKN8"
      },
      "outputs": [],
      "source": [
        "def compute_loss(params: hk.Params, graph: jraph.GraphsTuple, label: jnp.ndarray,\n",
        "                 net: jraph.GraphsTuple) -\u003e Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "  \"\"\"Computes loss and accuracy.\"\"\"\n",
        "  pred_graph = net.apply(params, graph)\n",
        "  preds = jax.nn.log_softmax(pred_graph.globals)\n",
        "  targets = jax.nn.one_hot(label, 2)\n",
        "\n",
        "  # Since we have an extra 'dummy' graph in our batch due to padding, we want\n",
        "  # to mask out any loss associated with the dummy graph.\n",
        "  # Since we padded with `pad_with_graphs` we can recover the mask by using\n",
        "  # get_graph_padding_mask.\n",
        "  mask = jraph.get_graph_padding_mask(pred_graph)\n",
        "\n",
        "  # Cross entropy loss.\n",
        "  loss = -jnp.mean(preds * targets * mask[:, None])\n",
        "\n",
        "  # Accuracy taking into account the mask.\n",
        "  accuracy = jnp.sum(\n",
        "      (jnp.argmax(pred_graph.globals, axis=1) == label) * mask) / jnp.sum(mask)\n",
        "  return loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OdXLDCibivA"
      },
      "source": [
        "#### Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8rTdn3Jbh9-"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/ogb_examples/train.py\n",
        "def train(dataset: List[Dict[str, Any]], num_train_steps: int) -\u003e hk.Params:\n",
        "  \"\"\"Training loop.\"\"\"\n",
        "\n",
        "  # Transform impure `net_fn` to pure functions with hk.transform.\n",
        "  net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "  # Get a candidate graph and label to initialize the network.\n",
        "  graph = dataset[0]['input_graph']\n",
        "\n",
        "  # Initialize the network.\n",
        "  params = net.init(jax.random.PRNGKey(42), graph)\n",
        "  # Initialize the optimizer.\n",
        "  opt_init, opt_update = optax.adam(1e-4)\n",
        "  opt_state = opt_init(params)\n",
        "\n",
        "  compute_loss_fn = functools.partial(compute_loss, net=net)\n",
        "  # We jit the computation of our loss, since this is the main computation.\n",
        "  # Using jax.jit means that we will use a single accelerator. If you want\n",
        "  # to use more than 1 accelerator, use jax.pmap. More information can be\n",
        "  # found in the jax documentation.\n",
        "  compute_loss_fn = jax.jit(jax.value_and_grad(\n",
        "      compute_loss_fn, has_aux=True))\n",
        "\n",
        "  for idx in range(num_train_steps):\n",
        "    graph = dataset[idx % len(dataset)]['input_graph']\n",
        "    label = dataset[idx % len(dataset)]['target']\n",
        "    # Jax will re-jit your graphnet every time a new graph shape is encountered.\n",
        "    # In the limit, this means a new compilation every training step, which\n",
        "    # will result in *extremely* slow training. To prevent this, pad each\n",
        "    # batch of graphs to the nearest power of two. Since jax maintains a cache\n",
        "    # of compiled programs, the compilation cost is amortized.\n",
        "    graph = pad_graph_to_nearest_power_of_two(graph)\n",
        "\n",
        "    # Since padding is implemented with pad_with_graphs, an extra graph has\n",
        "    # been added to the batch, which means there should be an extra label.\n",
        "    label = jnp.concatenate([label, jnp.array([0])])\n",
        "\n",
        "    (loss, acc), grad = compute_loss_fn(params, graph, label)\n",
        "    updates, opt_state = opt_update(grad, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    if idx % 50 == 0:\n",
        "      print(f'step: {idx}, loss: {loss}, acc: {acc}')\n",
        "  print('Training finished')\n",
        "  return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOuB_EtDZQrw"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataset: List[Dict[str, Any]],\n",
        "             params: hk.Params) -\u003e Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "  \"\"\"Evaluation Script.\"\"\"\n",
        "  # Transform impure `net_fn` to pure functions with hk.transform.\n",
        "  net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "  # Get a candidate graph and label to initialize the network.\n",
        "  graph = dataset[0]['input_graph']\n",
        "  accumulated_loss = 0\n",
        "  accumulated_accuracy = 0\n",
        "  compute_loss_fn = jax.jit(functools.partial(compute_loss, net=net))\n",
        "  for idx in range(len(dataset)):\n",
        "    graph = dataset[idx]['input_graph']\n",
        "    label = dataset[idx]['target']\n",
        "    graph = pad_graph_to_nearest_power_of_two(graph)\n",
        "    label = jnp.concatenate([label, jnp.array([0])])\n",
        "    loss, acc = compute_loss_fn(params, graph, label)\n",
        "    accumulated_accuracy += acc\n",
        "    accumulated_loss += loss\n",
        "    if idx % 100 == 0:\n",
        "      print(f'Evaluated {idx + 1} graphs')\n",
        "  print('Completed evaluation.')\n",
        "  loss = accumulated_loss / idx\n",
        "  accuracy = accumulated_accuracy / idx\n",
        "  print(f'Eval loss: {loss}, accuracy {accuracy}')\n",
        "  return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzCaMsTRZQti"
      },
      "outputs": [],
      "source": [
        "params = train(train_mutag_ds, num_train_steps=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6Yn4WX3c7ol"
      },
      "outputs": [],
      "source": [
        "evaluate(test_mutag_ds, params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjEWtqZ14GnM"
      },
      "source": [
        "We converge at ~76% test accuracy. We could of course further tune the parameters to improve this result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwVE88dTRC6V"
      },
      "source": [
        "## Link prediction on CORA (Citation Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP-j7Zq59uRZ"
      },
      "source": [
        "The final problem type we will explore is **link prediction**, an instance of an **edge-level** task. Given a graph, our goal is to predict whether a certain edge $(u,v)$ should be present or not. This is often useful in the recommender system settings (e.g., propose new friends in a social network, propose a movie to a user).\n",
        "\n",
        "As before, the first step is to obtain node latents $h_i$ using a GNN. In this context we will use the autoencoder language and call this GNN **encoder**. Then, we learn a binary classifier $f: (h_i, h_j) \\to z_{i,j}$ (**decoder**), predicting if an edge $(i,j)$ should exist or not. While we could use a more elaborate decoder (e.g., an MLP), a common approach we will also use here is to focus on obtaining good node embeddings, and for the decoder simply use the similarity between node latents, i.e. $z_{i,j} = h_i^T h_j$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bfxJdcNCeY7"
      },
      "source": [
        "For this problem we will use the [**Cora** dataset](https://linqs.github.io/linqs-website/datasets/#cora), a citation graph containing 2708 scientific publications. For each publication we have a 1433-dimensional feature vector, which is a bag-of-words representation (with a small, fixed dictionary) of the paper text. The edges in this graph represent citations, and are commonly treated as undirected. Each paper is in one of seven topics (classes) so you can also use this dataset for node classification.\n",
        "\n",
        "Similar to MUTAG, we have converted this dataset to jraph for you.\n",
        "\n",
        "Citation for the use of the Cora dataset:\n",
        "- [Qing Lu and Lise Getoor. Link-Based Classification. International Conference on Machine Learning. 2003.](https://linqs.github.io/linqs-website/publications/#id:lu-icml03)\n",
        "- [Sen, Prithviraj, et al. Collective classification in network data. AI magazine 29.3. 2008.](https://linqs.github.io/linqs-website/datasets/#cora)\n",
        "- [Dataset download link](https://linqs.github.io/linqs-website/datasets/#cora)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qo4jSfHtdQ9"
      },
      "outputs": [],
      "source": [
        "# Download jraph version of Cora.\n",
        "!wget -P /tmp/ https://storage.googleapis.com/dm-educational/assets/graph-nets/jraph_datasets/cora.pickle\n",
        "with open('/tmp/cora.pickle', 'rb') as f:\n",
        "  cora_ds = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lc0O4_h9Bfv"
      },
      "source": [
        "#### Splitting Edges and Adding \"Negative\" Edges\n",
        "For the link prediction task, we split the edges into train, val and test sets and also add \"negative\" examples (edges that do not correspond to a citation). We will ignore the topic classes.\n",
        "\n",
        "For the validation and test splits, we add the same number of existing edges (\"positive examples\") and non-existing edges (\"negative examples\").\n",
        "\n",
        "In contrast to the validation and test splits, the training split only contains positive examples (set $T_+$). The $|T_+|$ negative examples to be used during training will be sampled ad hoc in each epoch and uniformly at random from all edges that are not in $T_+$. This allows the model to see a wider range of negative examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqZyzwpC3p-u"
      },
      "outputs": [],
      "source": [
        "def train_val_test_split_edges(graph: jraph.GraphsTuple,\n",
        "                               val_perc: float = 0.05,\n",
        "                               test_perc: float = 0.1):\n",
        "  \"\"\"Split edges in input graph into train, val and test splits.\n",
        "\n",
        "  For val and test sets, also include negative edges.\n",
        "  Based on torch_geometric.utils.train_test_split_edges.\n",
        "  \"\"\"\n",
        "  mask = graph.senders \u003c graph.receivers\n",
        "  senders = graph.senders[mask]\n",
        "  receivers = graph.receivers[mask]\n",
        "  num_val = int(val_perc * senders.shape[0])\n",
        "  num_test = int(test_perc * senders.shape[0])\n",
        "  permuted_indices = onp.random.permutation(range(senders.shape[0]))\n",
        "  senders = senders[permuted_indices]\n",
        "  receivers = receivers[permuted_indices]\n",
        "  if graph.edges is not None:\n",
        "    edges = graph.edges[permuted_indices]\n",
        "\n",
        "  val_senders = senders[:num_val]\n",
        "  val_receivers = receivers[:num_val]\n",
        "  if graph.edges is not None:\n",
        "    val_edges = edges[:num_val]\n",
        "\n",
        "  test_senders = senders[num_val:num_val + num_test]\n",
        "  test_receivers = receivers[num_val:num_val + num_test]\n",
        "  if graph.edges is not None:\n",
        "    test_edges = edges[num_val:num_val + num_test]\n",
        "\n",
        "  train_senders = senders[num_val + num_test:]\n",
        "  train_receivers = receivers[num_val + num_test:]\n",
        "  train_edges = None\n",
        "  if graph.edges is not None:\n",
        "    train_edges = edges[num_val + num_test:]\n",
        "\n",
        "  # make training edges undirected by adding reverse edges back in\n",
        "  train_senders_undir = jnp.concatenate((train_senders, train_receivers))\n",
        "  train_receivers_undir = jnp.concatenate((train_receivers, train_senders))\n",
        "  train_senders = train_senders_undir\n",
        "  train_receivers = train_receivers_undir\n",
        "\n",
        "  # Negative edges.\n",
        "  num_nodes = graph.n_node[0]\n",
        "  # Create a negative adjacency mask, s.t. mask[i, j] = True iff edge i-\u003ej does\n",
        "  # not exist in the original graph.\n",
        "  neg_adj_mask = onp.ones((num_nodes, num_nodes), dtype=onp.uint8)\n",
        "  # upper triangular part\n",
        "  neg_adj_mask = onp.triu(neg_adj_mask, k=1)\n",
        "  neg_adj_mask[graph.senders, graph.receivers] = 0\n",
        "  neg_adj_mask = neg_adj_mask.astype(onp.bool)\n",
        "  neg_senders, neg_receivers = neg_adj_mask.nonzero()\n",
        "\n",
        "  perm = onp.random.permutation(range(len(neg_senders)))\n",
        "  neg_senders = neg_senders[perm]\n",
        "  neg_receivers = neg_receivers[perm]\n",
        "\n",
        "  val_neg_senders = neg_senders[:num_val]\n",
        "  val_neg_receivers = neg_receivers[:num_val]\n",
        "  test_neg_senders = neg_senders[num_val:num_val + num_test]\n",
        "  test_neg_receivers = neg_receivers[num_val:num_val + num_test]\n",
        "\n",
        "  train_graph = jraph.GraphsTuple(\n",
        "      nodes=graph.nodes,\n",
        "      edges=train_edges,\n",
        "      senders=train_senders,\n",
        "      receivers=train_receivers,\n",
        "      n_node=graph.n_node,\n",
        "      n_edge=jnp.array([len(train_senders)]),\n",
        "      globals=graph.globals)\n",
        "\n",
        "  return train_graph, neg_adj_mask, val_senders, val_receivers, val_neg_senders, val_neg_receivers, test_senders, test_receivers, test_neg_senders, test_neg_receivers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cE2CNtN-G3D"
      },
      "source": [
        "#### Test the Edge Splitting Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhBruorYJPm6"
      },
      "outputs": [],
      "source": [
        "graph = cora_ds[0]['input_graph']\n",
        "train_graph, neg_adj_mask, val_pos_senders, val_pos_receivers, val_neg_senders, val_neg_receivers, test_pos_senders, test_pos_receivers, test_neg_senders, test_neg_receivers = train_val_test_split_edges(graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOMs84n4Gz1e"
      },
      "outputs": [],
      "source": [
        "print(f'Train set: {train_graph.senders.shape[0]} positive edges, we will sample the same number of negative edges at runtime')\n",
        "print(f'Val set: {val_pos_senders.shape[0]} positive edges, {val_neg_senders.shape[0]} negative edges')\n",
        "print(f'Test set: {test_pos_senders.shape[0]} positive edges, {test_neg_senders.shape[0]} negative edges')\n",
        "print(f'Negative adjacency mask shape: {neg_adj_mask.shape}')\n",
        "print(f'Numbe of negative edges to sample from: {neg_adj_mask.sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzwvygx-BS6e"
      },
      "source": [
        "\n",
        "*Note*: It will often happen during training that as a negative example, we sample an initially existing edge (that is now e.g. a positive example in the test set). We are however not allowed to check for this, as we should be unaware of the existence of test edges during training.\n",
        "\n",
        "Assuming our dot product decoder, we are essentially attempting to bring the latents of endpoints of edges from $T_+$ closer together, and make the latents of all other pairs of nodes as distant as possible. As this is impossible to fully satisfy, the hope is that the model will \"fail\" to distance those pairs of nodes where the edges should actually exist (positive examples from the test set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaGRJAyfLwed"
      },
      "source": [
        "#### Graph Network Model Definition\n",
        "\n",
        "We will use jraph.GraphNetwork to build our graph net model.\n",
        "\n",
        "We first define update functions for node features. We are not using edge or global features for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC3AMCrJLwek"
      },
      "outputs": [],
      "source": [
        "@jraph.concatenated_args\n",
        "def node_update_fn(feats: jnp.ndarray) -\u003e jnp.ndarray:\n",
        "  \"\"\"Node update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.Linear(128), jax.nn.relu, hk.Linear(64)])\n",
        "  return net(feats)\n",
        "\n",
        "\n",
        "def net_fn(graph: jraph.GraphsTuple) -\u003e jraph.GraphsTuple:\n",
        "  \"\"\"Network definition.\"\"\"\n",
        "  graph = graph._replace(globals=jnp.zeros([graph.n_node.shape[0], 1]))\n",
        "  net = jraph.GraphNetwork(\n",
        "      update_node_fn=node_update_fn, update_edge_fn=None, update_global_fn=None)\n",
        "  return net(graph)\n",
        "\n",
        "\n",
        "def decode(pred_graph: jraph.GraphsTuple, senders: jnp.ndarray,\n",
        "           receivers: jnp.ndarray) -\u003e jnp.ndarray:\n",
        "  \"\"\"Given a set of candidate edges, take dot product of respective nodes.\n",
        "\n",
        "  Args:\n",
        "    pred_graph: input graph.\n",
        "    senders: Senders of candidate edges.\n",
        "    receivers: Receivers of candidate edges.\n",
        "\n",
        "  Returns:\n",
        "    For each edge, computes dot product of the features of the two nodes.\n",
        "\n",
        "  \"\"\"\n",
        "  return jnp.squeeze(\n",
        "      jnp.sum(pred_graph.nodes[senders] * pred_graph.nodes[receivers], axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmSjMB_TGAUE"
      },
      "source": [
        "To evaluate our model, we first apply the sigmoid function to obtained dot products to get a score $s_{i,j} \\in [0,1]$ for each edge. Now, we can pick a threshold $\\tau$ and say that we predict all pairs $(i,j)$ s.t. $s_{i,j} \\geq \\tau$ as edges (and all the rest as non-edges)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGz7iRGmLwel"
      },
      "source": [
        "#### Loss and ROC-AUC-Metric Function\n",
        "Define the binary classification cross-entropy loss.\n",
        "To aggregate the results over all choices of $\\tau$, we will use ROC-AUC (the area under the ROC curve) as our evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ld4b3D6Lwel"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_bce_with_logits_loss(x: jnp.ndarray, y: jnp.ndarray) -\u003e jnp.ndarray:\n",
        "  \"\"\"Computes binary cross-entropy with logits loss.\n",
        "\n",
        "  Combines sigmoid and BCE, and uses log-sum-exp trick for numerical stability.\n",
        "  See https://stackoverflow.com/a/66909858 if you want to learn more.\n",
        "\n",
        "  Args:\n",
        "    x: Predictions (logits).\n",
        "    y: Labels.\n",
        "\n",
        "  Returns:\n",
        "    Binary cross-entropy loss with mean aggregation.\n",
        "\n",
        "  \"\"\"\n",
        "  max_val = jnp.clip(x, 0, None)\n",
        "  loss = x - x * y + max_val + jnp.log(\n",
        "      jnp.exp(-max_val) + jnp.exp((-x - max_val)))\n",
        "  return loss.mean()\n",
        "\n",
        "\n",
        "def compute_loss(params: hk.Params, graph: jraph.GraphsTuple,\n",
        "                 senders: jnp.ndarray, receivers: jnp.ndarray,\n",
        "                 labels: jnp.ndarray,\n",
        "                 net: hk.Transformed) -\u003e Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "  \"\"\"Computes loss.\"\"\"\n",
        "  pred_graph = net.apply(params, graph)\n",
        "  preds = decode(pred_graph, senders, receivers)\n",
        "  loss = compute_bce_with_logits_loss(preds, labels)\n",
        "  return loss, preds\n",
        "\n",
        "\n",
        "def compute_roc_auc_score(preds: jnp.ndarray,\n",
        "                          labels: jnp.ndarray) -\u003e jnp.ndarray:\n",
        "  \"\"\"Computes roc auc (area under the curve) score for classification.\"\"\"\n",
        "  s = jax.nn.sigmoid(preds)\n",
        "  roc_auc = roc_auc_score(labels, s)\n",
        "  return roc_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58D3mm9blJFD"
      },
      "source": [
        "Helper function for sampling negative edges during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTZ4CwVDWrc9"
      },
      "outputs": [],
      "source": [
        "def negative_sampling(\n",
        "    graph: jraph.GraphsTuple, num_neg_samples: int,\n",
        "    key: jnp.DeviceArray) -\u003e Tuple[jnp.DeviceArray, jnp.DeviceArray]:\n",
        "  \"\"\"Samples negative edges, i.e. edges that don't exist in the input graph.\"\"\"\n",
        "  num_nodes = graph.n_node[0]\n",
        "  total_possible_edges = num_nodes**2\n",
        "  # convert 2D edge indices to 1D representation.\n",
        "  pos_idx = graph.senders * num_nodes + graph.receivers\n",
        "\n",
        "  # Percentage to oversample edges, so most likely will sample enough neg edges.\n",
        "  alpha = jnp.abs(1 / (1 - 1.1 *\n",
        "                       (graph.senders.shape[0] / total_possible_edges)))\n",
        "\n",
        "  perm = jax.random.randint(\n",
        "      key,\n",
        "      shape=(int(alpha * num_neg_samples),),\n",
        "      minval=0,\n",
        "      maxval=total_possible_edges,\n",
        "      dtype=jnp.uint32)\n",
        "\n",
        "  # mask where sampled edges are positive edges.\n",
        "  mask = jnp.isin(perm, pos_idx)\n",
        "  # remove positive edges.\n",
        "  perm = perm[~mask][:num_neg_samples]\n",
        "\n",
        "  # convert 1d back to 2d edge indices.\n",
        "  neg_senders = perm // num_nodes\n",
        "  neg_receivers = perm % num_nodes\n",
        "\n",
        "  return neg_senders, neg_receivers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5xUW5XkL3YS"
      },
      "source": [
        "Let's write the training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAjfWFduSESI"
      },
      "outputs": [],
      "source": [
        "def train(dataset: List[Dict[str, Any]], num_epochs: int) -\u003e hk.Params:\n",
        "  \"\"\"Training loop.\"\"\"\n",
        "  key = jax.random.PRNGKey(42)\n",
        "  # Transform impure `net_fn` to pure functions with hk.transform.\n",
        "  net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "  # Get a candidate graph and label to initialize the network.\n",
        "  graph = dataset[0]['input_graph']\n",
        "\n",
        "  train_graph, _, val_pos_s, val_pos_r, val_neg_s, val_neg_r, test_pos_s, \\\n",
        "      test_pos_r, test_neg_s, test_neg_r = train_val_test_split_edges(\n",
        "      graph)\n",
        "\n",
        "  # Prepare the validation and test data.\n",
        "  val_senders = jnp.concatenate((val_pos_s, val_neg_s))\n",
        "  val_receivers = jnp.concatenate((val_pos_r, val_neg_r))\n",
        "  val_labels = jnp.concatenate(\n",
        "      (jnp.ones(len(val_pos_s)), jnp.zeros(len(val_neg_s))))\n",
        "  test_senders = jnp.concatenate((test_pos_s, test_neg_s))\n",
        "  test_receivers = jnp.concatenate((test_pos_r, test_neg_r))\n",
        "  test_labels = jnp.concatenate(\n",
        "      (jnp.ones(len(test_pos_s)), jnp.zeros(len(test_neg_s))))\n",
        "  # Initialize the network.\n",
        "  params = net.init(key, train_graph)\n",
        "  # Initialize the optimizer.\n",
        "  opt_init, opt_update = optax.adam(1e-4)\n",
        "  opt_state = opt_init(params)\n",
        "\n",
        "  compute_loss_fn = functools.partial(compute_loss, net=net)\n",
        "  # We jit the computation of our loss, since this is the main computation.\n",
        "  # Using jax.jit means that we will use a single accelerator. If you want\n",
        "  # to use more than 1 accelerator, use jax.pmap. More information can be\n",
        "  # found in the jax documentation.\n",
        "  compute_loss_fn = jax.jit(jax.value_and_grad(compute_loss_fn, has_aux=True))\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    num_neg_samples = train_graph.senders.shape[0]\n",
        "    train_neg_senders, train_neg_receivers = negative_sampling(\n",
        "        train_graph, num_neg_samples=num_neg_samples, key=key)\n",
        "    train_senders = jnp.concatenate((train_graph.senders, train_neg_senders))\n",
        "    train_receivers = jnp.concatenate(\n",
        "        (train_graph.receivers, train_neg_receivers))\n",
        "    train_labels = jnp.concatenate(\n",
        "        (jnp.ones(len(train_graph.senders)), jnp.zeros(len(train_neg_senders))))\n",
        "\n",
        "    (train_loss,\n",
        "     train_preds), grad = compute_loss_fn(params, train_graph, train_senders,\n",
        "                                          train_receivers, train_labels)\n",
        "\n",
        "    updates, opt_state = opt_update(grad, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    if epoch % 10 == 0 or epoch == (num_epochs - 1):\n",
        "      train_roc_auc = compute_roc_auc_score(train_preds, train_labels)\n",
        "      val_loss, val_preds = compute_loss(params, train_graph, val_senders,\n",
        "                                         val_receivers, val_labels, net)\n",
        "      val_roc_auc = compute_roc_auc_score(val_preds, val_labels)\n",
        "      print(f'epoch: {epoch}, train_loss: {train_loss:.3f}, '\n",
        "            f'train_roc_auc: {train_roc_auc:.3f}, val_loss: {val_loss:.3f}, '\n",
        "            f'val_roc_auc: {val_roc_auc:.3f}')\n",
        "  test_loss, test_preds = compute_loss(params, train_graph, test_senders,\n",
        "                                       test_receivers, test_labels, net)\n",
        "  test_roc_auc = compute_roc_auc_score(test_preds, test_labels)\n",
        "  print('Training finished')\n",
        "  print(\n",
        "      f'epoch: {epoch}, test_loss: {test_loss:.3f}, test_roc_auc: {test_roc_auc:.3f}'\n",
        "  )\n",
        "  return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ealPntyvdxyT"
      },
      "source": [
        "Let's train the model! We expect the model to reach roughly test_roc_auc of 0.84.\n",
        "\n",
        "(Note that ROC-AUC is a scalar between 0 and 1, with 1 being the ROC-AUC of a perfect classifier.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LJtk4WKenoE"
      },
      "outputs": [],
      "source": [
        "params = train(cora_ds, num_epochs=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35kbP8GZRFEm"
      },
      "source": [
        "## [Bonus] Intro to Graph Adversarial Attacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnqZxqtEaKoi"
      },
      "source": [
        "It is known that deep neural networks for vision can be easily fooled by **imperceptible** transformations of the input:\n",
        "\n",
        "\u003cimage src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/cnn_attack.png\" width=\"600px\"\u003e\n",
        "\n",
        "Figure credit: https://openai.com/blog/adversarial-example-research/, adapted from [Goodfellow et al. (2015)](https://arxiv.org/pdf/1412.6572.pdf) (page 3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5GI0EW4a3_B"
      },
      "source": [
        " In this setting, we often define imperceptibility, i.e., the attacker budget, as some neighborhood of the input image. For example, assuming the input image $x$ has pixel values in $[0,1]$, we might look for adversarial attacks inside the $l_\\infty$ ball of radius $0.1$, i.e., *adversarial inputs* $x'$ s.t. $||x-x'||_\\infty \u003c 0.1$ on which the network makes a mistake.\n",
        "\n",
        "Unsuprisingly, this extends to various other domains (language, sound, etc.), as well as graphs -- tweaking the features of a node or adding/removing an edge can easily confuse a GNN model:\n",
        "\n",
        "\u003cimage src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/perturbed_graph.png\" width=\"500px\"\u003e\n",
        "\n",
        "Figure credit:  https://github.com/DSE-MSU/DeepRobust (adversary_examples/graph_attack_example.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zHSCHCPcMPG"
      },
      "source": [
        "However, there are several challenges specific to graphs that make adversarial attacks and defenses interesting for GNNs:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8b7gWSSchoB"
      },
      "source": [
        "1. **The graphs are discrete** -- as pixel values are continuous, perturbing them almost always leads to a valid input image. However, perturbing adjacency matrices is more involved as they need to remain discrete, which also makes gradient-based methods significantly harder to directly apply."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g-AGvkKfLP1"
      },
      "source": [
        "2. **The nodes in the graph are not independent** -- by the nature of GNNs, perturbing the features of one node might affect the prediction of another. This led to new notions such as *influencer attacks* where we attempt to make a GNN misclassify a node without directly manipulating it in any way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzaGfMGmgh-q"
      },
      "source": [
        "3. **It is difficult to define imperceptibility** -- while in images this comes naturally and can be easily formalized by thinking about $l_\\infty$ or $l_2$ neighborhoods, it is unclear how this generalizes to graphs. Various definitions have been proposed (e.g., the attack should preserve some graph properties such as the degree distribution), but also used to craft *graph purification* procedures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdTXgDi3hD5V"
      },
      "source": [
        "These unique challenges lead to a very active research field: follow [this repo](https://github.com/safe-graph/graph-adversarial-learning-literature) for most recent papers. We also recommend [DeepRobust](https://github.com/DSE-MSU/DeepRobust), which offers implementations of various attack/defense papers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S8X2UovqE-I"
      },
      "source": [
        "### Summary\n",
        "In this tutorial, we learned about how to represent directed and undirected graphs in jraph, how to create the basic GNN blocks (GCN + GAT) and how to compose them into GNNs. We also learned about three types of graph predictions tasks (node classification, link prediction and graph classification) and trained a model on each of these tasks.\n",
        "\n",
        "\n",
        "To practice your knowledge, you can:\n",
        "* Create your own graphs, e.g. following the toy graph template and adding nodes, edges, features.\n",
        "* Modify the GNN blocks, e.g. by changing the `update_node_fn()` / `update_edge_fn()` functions.\n",
        "* Train a model on a different dataset and/or task."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Introduction to Graph Neural Nets with JAX/jraph",
      "private_outputs": true,
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
