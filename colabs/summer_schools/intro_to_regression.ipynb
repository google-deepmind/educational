{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to Supervised Learning 1 - Regression",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqAbjlWES9Tf"
      },
      "source": [
        "> <p><small><small>Copyright 2022 DeepMind Technologies Limited.</p>\n",
        "> <p><small><small> Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at </p>\n",
        "> <p><small><small> <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">https://www.apache.org/licenses/LICENSE-2.0</a> </p>\n",
        "> <p><small><small> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVWXpe45sbB-"
      },
      "source": [
        "# **Introduction to Supervised Learning 1 - Regression**\n",
        "<a href=\"https://github.com/deepmind/educational/tree/master/colabs/summer_schools/intro_to_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "Welcome to a practical session that will teach you a few basic concepts used across modern machine learning.\n",
        "The practical assumes prior knowledge of NumPy, as well as basic linear algebra.\n",
        "\n",
        "**Learning objectives**\n",
        "\n",
        "This practical is designed to help you see the wood (some basic concepts in supervised learning) from the trees (the ever growing body of approaches).\n",
        "In this practical you will predict a real-valued output $y$ from a scalar input $x$.\n",
        "Don't worry, you will encounter multivariate inputs soon in the next practical (Introduction to Supervised Learning 2 - Classification).\n",
        "\n",
        "In this practical, you will:\n",
        "- learn what a model for data is and what model parameters are;\n",
        "- experience modeling assumptions;\n",
        "- experience basic optimization of a loss function to fit data;\n",
        "- get introduced to automatic differentiation;\n",
        "- extend linear features to nonlinear features in a linear model;\n",
        "- learn about underfitting and overfitting and basic regularization.\n",
        "\n",
        "**Disclaimer**\n",
        "\n",
        "This code is intended for educational purposes, and in the name of readability for a non-technical audience does not always follow best practices for software engineering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-LJk6UAOqJi"
      },
      "source": [
        "# **Linear Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfzWUUuhqKSC"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format=\"retina\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Let's plot basic data!**"
      ],
      "metadata": {
        "id": "inK7Ny0Xqyqo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcHizmUMty_c"
      },
      "source": [
        "x_data_list = [1, 2, 3, 4, 5]\n",
        "y_data_list = [3, 2, 3, 1, 0]\n",
        "\n",
        "def plot_basic_data(parameters_list=None, title=\"Observed data\"):\n",
        "  xlim = [-1, 7]\n",
        "  fig, ax = plt.subplots()\n",
        "  \n",
        "  if parameters_list is not None:\n",
        "    x_pred = np.linspace(xlim[0], xlim[1], 100)\n",
        "    for parameters in parameters_list:\n",
        "      y_pred = parameters[0] + parameters[1] * x_pred\n",
        "      ax.plot(x_pred, y_pred, ':', color=[1, 0.7, 0.6])\n",
        "\n",
        "    parameters = parameters_list[-1]\n",
        "    y_pred = parameters[0] + parameters[1] * x_pred\n",
        "    ax.plot(x_pred, y_pred, \"-\", color=[1, 0, 0], lw=2)\n",
        "\n",
        "  ax.plot(x_data_list, y_data_list, \"ob\")\n",
        "  ax.set(xlabel=\"Input x\", ylabel=\"Output y\",\n",
        "         title=title,\n",
        "         xlim=xlim, ylim=[-2, 5])\n",
        "  ax.grid()\n",
        "\n",
        "plot_basic_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQnbbnkn1od1"
      },
      "source": [
        "parameters_list = []   # A list of parameters for the next exercise."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGXCFCfz2vH8"
      },
      "source": [
        "## **Tuning parameters by hand...**\n",
        "\n",
        "Above you can see some data points where we have outputs for each input.\n",
        "We want to predict output $y$ given input values for $x$.\n",
        "We start by modelling the data with a simple linear function $f(x) = \\color{red}{w} x + \\color{red}{b}$.\n",
        "There are two numbers, $\\color{red}{b}$ and $\\color{red}{w}$, which we call the model's parameters.\n",
        "If we change them, then $f(x)$ will change accordingly!\n",
        "\n",
        "Your next challenge is to find a \"good\" setting of parameters $\\color{red}{b}$ and $\\color{red}{w}$ by hand.\n",
        "\"But I came here to learn about deep learning!\" we hear you say. True.\n",
        "But we are going to start small, and after this manual exercise, we'll ask you questions about assumptions that you made that you didn't even know you made! Ready? Ready!\n",
        "\n",
        "**Exercise: Finding two \"good\" parameters**\n",
        "1. Move the two sliders below to set $\\color{red}{b}$ and $\\color{red}{w}$, and press \"Run cell\" on the code cell below. \n",
        "2. Is your $f(x)$ close to the blue data points? Can you find a better fit?\n",
        "3. Adjust the two sliders a bit more, and press \"Run cell\" again on the cell...\n",
        "4. If your $f(x)$ now closer to all the blue data points? Repeat and repeat step 3 until you get a manual fit that you are happy with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWcRQXpIxtuq"
      },
      "source": [
        "b = 0.74 #@param {type:\"slider\", min:-1, max:8, step:0.01}\n",
        "w = 0.88 #@param {type:\"slider\", min:-3, max:3, step:0.01}\n",
        "print(\"Plotting line\", w, \"* x +\", b)\n",
        "parameters = [b, w]\n",
        "parameters_list.append(parameters)\n",
        "plot_basic_data(parameters_list,\n",
        "                title=\"Observed data and my first predictions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0Fn0LfBCf80"
      },
      "source": [
        "## **Weights and biases**\n",
        "What happened to the function when you changed $\\color{red}{b}$?\n",
        "And when you changed $\\color{red}{w}$? It's the intercept and slope, we hear you say!\n",
        "We picked notation \"$\\color{red}{w}$\" and \"$\\color{red}{b}$\" for a reason, as our models will become more complicated than linear functions!\n",
        "- \"$\\color{red}{w}$\" stands for \"weight\", which is multiplied with $x$ (or in more complicated models, other functions of $x$).\n",
        "- \"$\\color{red}{b}$\" stands for \"bias\". It shifts the line up or down in the absence of any data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk1IWNZsbdop"
      },
      "source": [
        "##**You're a born optimizer!**\n",
        "\n",
        "We will now plot your sequence of choices for $\\color{red}{b}$ and $\\color{red}{w}$ on a $(\\color{red}{b}, \\color{red}{w})$ axis. Press \"run\" on the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPIW4JTzDW7u"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "opt = {\"head_width\": 0.2, \"head_length\": 0.2,\n",
        "       \"length_includes_head\": True, \"color\": \"r\"}\n",
        "if parameters_list is not None:\n",
        "  b_old = parameters_list[0][0]\n",
        "  w_old = parameters_list[0][1]\n",
        "  for i in range(1, len(parameters_list)):\n",
        "    b_next = parameters_list[i][0]\n",
        "    w_next = parameters_list[i][1]\n",
        "    ax.arrow(b_old, w_old, b_next - b_old, w_next - w_old, **opt)\n",
        "    b_old, w_old = b_next, w_next\n",
        "\n",
        "  ax.scatter(b_old, w_old, s=200, marker=\"o\", color=\"y\")\n",
        "  bs = [parameters[0] for parameters in parameters_list]\n",
        "  ws =  [parameters[1] for parameters in parameters_list]\n",
        "  ax.scatter(bs, ws, s=40, marker='o', color='k')\n",
        "\n",
        "ax.set(xlabel=\"Bias b\", ylabel=\"Weight w\",\n",
        "       title=\"My sequence of b\\'s and w\\'s\",\n",
        "       xlim=[-1, 8], ylim=[-3, 3])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_5cpvZFJDWo"
      },
      "source": [
        "## **Is your neighbour a born optimizer?**\n",
        "\n",
        "Look at the plot of your sequence of choices for $\\color{red}{b}$ and $\\color{red}{w}$.\n",
        "Do you notice how they changed? If you're doing this practical in a group, pause here and compare your solution with that of your neighbours:\n",
        "\n",
        "- Did they change $\\color{red}{b}$ and $\\color{red}{w}$ with big steps or small steps each time?\n",
        "- Did they start with small steps, and then progressed to bigger steps? Or the other way round? What about you?\n",
        "- Did the magnitude of your previous steps influence your next choice? Why? Or why not?\n",
        "- Did you all converge to roughly the same endpoint for $\\color{red}{b}$ and $\\color{red}{w}$, or did your sequences end up in different places?\n",
        "\n",
        "## **Did you make any assumptions?**\n",
        "\n",
        "Every model makes assumptions. One assumtion that we made is that our model is a *linear* model, i.e. that our best guess is for $y$ is with $f(x) = \\color{red}{w} x + \\color{red}{b}$.\n",
        "Turn to your neighbours and tell them how you would approach guessing $\\color{red}{b}$ and $\\color{red}{w}$ if:\n",
        "- someone paid you a million dollars if you predicted $y$ correctly at $x = 4.9$, but paid you nothing if you predicted $y$ correctly at $x = 1.1$;\n",
        "- if one of the entries for $y$ actually contained a typo?\n",
        "\n",
        "Would your solution change? Why? Or why not?\n",
        "\n",
        "## **A loss function**\n",
        "\n",
        "You tweaked two numbers (or parameters), $\\color{red}{b}$ and $\\color{red}{w}$, to find a good fit.\n",
        "We want to define a function called a \"loss function\" that is at its smallest when *you* think the data fit is great, and that is big when the data fit is not so great.\n",
        "\n",
        "Below, we've created a loss function $\\mathrm{loss}(\\color{red}{b}, \\color{red}{w})$.\n",
        "Is it small at the best $\\color{red}{b}$ and $\\color{red}{w}$ that you found manually?\n",
        "Your sequence of choices for $\\color{red}{b}$ and $\\color{red}{w}$ are also plotted on the $(\\color{red}{b}, \\color{red}{w})$ axis.\n",
        "Does your sequence progressively move toward a parameter setting for which the loss function is small?\n",
        "We plotted two views of the loss function, so that it is easier to see the minimum *and* the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IwNXqQEgrCr"
      },
      "source": [
        "def l1_loss(b, w):\n",
        "  loss = 0 * b\n",
        "  for x, y in zip(x_data_list, y_data_list):\n",
        "    f = w * x + b\n",
        "    loss += np.abs(f - y)\n",
        "  return loss / len(x_data_list)\n",
        "\n",
        "bs, ws = np.linspace(-1, 8, num=25), np.linspace(-3, 3, num=25)\n",
        "b_grid, w_grid = np.meshgrid(bs, ws)\n",
        "loss_grid = l1_loss(b_grid, w_grid)\n",
        "\n",
        "def plot_loss(parameters_list, title, show_stops=False):\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(18, 8),\n",
        "                         subplot_kw={\"projection\": \"3d\"})\n",
        "  ax[0].view_init(10, -30)\n",
        "  ax[1].view_init(30, -30)\n",
        "\n",
        "  if parameters_list is not None:\n",
        "    b_old = parameters_list[0][0]\n",
        "    w_old = parameters_list[0][1]\n",
        "    loss_old = l1_loss(b_old, w_old)\n",
        "    ls = [loss_old]\n",
        "\n",
        "    for i in range(1, len(parameters_list)):\n",
        "      b_next = parameters_list[i][0]\n",
        "      w_next = parameters_list[i][1]\n",
        "      loss_next = l1_loss(b_next, w_next)\n",
        "      ls.append(loss_next)\n",
        "\n",
        "      ax[0].plot([b_old, b_next], [w_old, w_next], [loss_old, loss_next],\n",
        "                color=\"red\", alpha=0.8, lw=2)\n",
        "      ax[1].plot([b_old, b_next], [w_old, w_next], [loss_old, loss_next],\n",
        "                color=\"red\", alpha=0.8, lw=2)\n",
        "      b_old, w_old, loss_old = b_next, w_next, loss_next\n",
        "\n",
        "    if show_stops:\n",
        "      ax[0].scatter(b_old, w_old, loss_old, s=100, marker=\"o\", color=\"y\")\n",
        "      ax[1].scatter(b_old, w_old, loss_old, s=100, marker=\"o\", color=\"y\")\n",
        "      bs = [parameters[0] for parameters in parameters_list]\n",
        "      ws = [parameters[1] for parameters in parameters_list]\n",
        "      ax[0].scatter(bs, ws, ls, s=40, marker=\"o\", color=\"k\")\n",
        "      ax[1].scatter(bs, ws, ls, s=40, marker=\"o\", color=\"k\")\n",
        "    else:\n",
        "      ax[0].scatter(b_old, w_old, loss_old, s=40, marker='o', color='k')\n",
        "      ax[1].scatter(b_old, w_old, loss_old, s=40, marker='o', color='k')\n",
        "\n",
        "  ax[0].plot_surface(b_grid, w_grid, loss_grid, cmap=cm.coolwarm,\n",
        "                     linewidth=0, alpha=0.4, antialiased=False)\n",
        "  ax[1].plot_surface(b_grid, w_grid, loss_grid, cmap=cm.coolwarm,\n",
        "                     linewidth=0, alpha=0.4, antialiased=False)\n",
        "  ax[0].set(xlabel=\"Bias b\", ylabel=\"Weight w\", zlabel=\"Loss\", title=title)\n",
        "  ax[1].set(xlabel=\"Bias b\", ylabel=\"Weight w\", zlabel=\"Loss\", title=title)\n",
        "  plt.show()\n",
        "\n",
        "plot_loss(parameters_list,\n",
        "          \"An example loss function and my sequence of b\\'s and w\\'s\",\n",
        "          show_stops=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qperTSozDKAp"
      },
      "source": [
        "## **Let's contruct a loss function**\n",
        "\n",
        "When you manually adjusted your weights $\\color{red}{b}$ and $\\color{red}{w}$, you probably looked at how close each $f(x)$ was to the $y$ that it tries to predict.\n",
        "Maybe you glanced at the distance from the red line to each of the blue dots, and imagined the average of the distances (marked in purple) below.\n",
        "If the average was small, your fit was good!\n",
        "\n",
        "<img src='https://storage.googleapis.com/dm-educational/assets/supervised-learning/loss-function-intro.svg' width='400'>\n",
        "\n",
        "To formalize this notion, let $x_1 = 1$, $x_2 = 2$, $x_3 = 3$... and let $y_1 = 3$, $y_2 = 2$, $y_3 = 3$... The blue dots are therefore a sequence of input-output $(x, y)$ pairs.\n",
        "Assuming that the order of the data points doesn't matter, and $n = 1, ..., N$ (where $N=5$ in our case) indexes the data, our loss will look something like this:\n",
        "\n",
        "$\\mathrm{loss}(\\color{red}{b}, \\color{red}{w}) = \\frac{1}{N} \\sum_{n=1}^N \\mathrm{error}(\\color{red}{b}, \\color{red}{w} ; x_n, y_n)$.\n",
        "\n",
        "We take all 5 purple bars (or errors), add them together, and then divide by 5 to get the average amount of purple ink used.\n",
        "So far, we've said nothing about $\\mathrm{error}(\\color{red}{b}, \\color{red}{w} ; x_n, y_n)$, except that:\n",
        "- all the error terms depend on $\\color{red}{b}$ and $\\color{red}{w}$;\n",
        "- each term only considers one data point $(x_n, y_n)$;\n",
        "- it doesn't matter in which order we sum the purple bars.\n",
        "\n",
        "What would you like the \"error\" to be? Choices, choices! We can just let it be the average of the purple distances,\n",
        "\n",
        "$\\mathrm{loss}(\\color{red}{b}, \\color{red}{w}) = \\frac{1}{N} \\sum_{n=1}^N \\Big|y_n - \\underbrace{(\\color{red}{w} x_n + \\color{red}{b})}_{f(x_n)} \\Big|$.\n",
        "\n",
        "We can also let the error be the average of the *squared* distances, also called the \"mean squared error\" (MSE).\n",
        "We'll write the mean squared error in pencil in light gray for now, as we'll return to that choice later.\n",
        "\n",
        "$\\color{lightgray}{\\mathrm{loss}(\\color{red}{b}, \\color{red}{w}) = \\frac{1}{N} \\sum_{n=1}^N \\Big(y_n - \\underbrace{(\\color{red}{w} x_n + \\color{red}{b})}_{f(x_n)} \\Big)^2}$.\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "Before we proceed, take a pencil and paper and draw a figure to illustrate the two error terms for one data point.\n",
        "Let the horizontal axis be $f(x)$ and let the vertical axis be $\\mathrm{error}(\\color{red}{b}, \\color{red}{w} ; x, y)$ for one data point.\n",
        "- For which value of $f(x)$ is each error minimized?\n",
        "- Explain to your neighbour how the penalties that the error terms give differ when $f(x)$ is close to $y$, and when $f(x)$ is far from $y$.\n",
        "- Can you design your own error term? Explain to your neighbour or your tutor the motivation behind your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZfDxmzQJ63U"
      },
      "source": [
        "## **Gradients**\n",
        "\n",
        "When you manually tweaked $\\color{red}{b}$ and $\\color{red}{w}$ to adjust the red line, you tried to adjust it so that the fit would be better.\n",
        "If you were an experienced manual parameter adjuster, you might even have adjusted the $\\color{red}{b}$ and $\\color{red}{w}$ so that the fit gets *maximally better* with each adjustment.\n",
        "That notion is the **direction of maximum decrease** of the loss function, or minus the gradient.\n",
        "\n",
        "Let's take the sum of absolute values, $\\mathrm{loss}(\\color{red}{b}, \\color{red}{w}) = \\frac{1}{N} \\sum_{n=1}^N |\\color{red}{w} x_n + \\color{red}{b} - y_n|$.\n",
        "We want to know in which direction to adjust $\\color{red}{b}$ **and** in which direction to adjust $\\color{red}{w}$.\n",
        "We require two gradients,\n",
        "\n",
        "$\\nabla_{\\color{red}{b}} \\mathrm{loss}(\\color{red}{b}, \\color{red}{w}) = \\frac{1}{N} \\sum_{n=1}^N\n",
        "\\begin{cases}\n",
        "    -1 & \\text{if } \\color{red}{w} x_n + \\color{red}{b} - y_n < 0 \\\\\n",
        "    1 & \\text{if } \\color{red}{w} x_n + \\color{red}{b} - y_n \\ge 0\n",
        "\\end{cases}$ ,\n",
        "\n",
        "$\\nabla_{\\color{red}{w}} \\mathrm{loss}(\\color{red}{b}, \\color{red}{w}) = \\frac{1}{N} \\sum_{n=1}^N\n",
        "\\begin{cases}\n",
        "    -x_n & \\text{if } \\color{red}{w} x_n + \\color{red}{b} - y_n < 0 \\\\\n",
        "    x_n & \\text{if } \\color{red}{w} x_n + \\color{red}{b} - y_n \\ge 0\n",
        "\\end{cases}$ .\n",
        "\n",
        "*(For the mathematical purists, yes: the gradient of the absolute value is technically not defined at zero, but we'll ignore that technicality for the purposes of finding a minimum for now.)*\n",
        "\n",
        "In the code snippet below, we compute the two gradients using a for-loop over examples.\n",
        "This is just to illustrate how the gradient is computed. Very soon, we'll throw away the for-loop over data points and do it \"all at once\" in vectorized operations!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOoseWuCT3NM"
      },
      "source": [
        "def manual_grad(b, w):\n",
        "  grad_b = 0\n",
        "  grad_w = 0\n",
        "  for x, y in zip(x_data_list, y_data_list):\n",
        "    f = w * x + b\n",
        "    grad_b += np.sign(f - y)\n",
        "    grad_w += np.sign(f - y) * x\n",
        "  grad_b /= len(x_data_list)\n",
        "  grad_w /= len(x_data_list)\n",
        "  return grad_b, grad_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omrnDANYaxT4"
      },
      "source": [
        "## **Gradient descent: No more tuning parameters by hand!**\n",
        "\n",
        "To get to the minimum of the loss function -- which hopefully gives us the data fit we want -- we want to repeatedly use the gradients to tweak the parameters $\\color{red}{b}$ and $\\color{red}{w}$ in the right direction.\n",
        "But how? That opens a whole direction of research! The simplest idea is to start with an initial guess $\\color{red}{b}$, and then repeatedly update\n",
        "\n",
        "$\\color{red}{b} \\leftarrow \\color{red}{b} - \\color{blue}{\\eta} \\nabla_{\\color{red}{b}} \\mathrm{loss}(\\color{red}{b}, \\color{red}{w})$ ,\n",
        "\n",
        "while also doing the same for $\\color{red}{w}$.\n",
        "The parameter $\\color{blue}{\\eta}$ just tells us how much we are going to scale the gradient before we use it to update our parameters:\n",
        "are we going to try to walk downhill with big steps or small steps?\n",
        "It is called a **learning rate**.\n",
        "\n",
        "**Exercise**\n",
        "1. Run the code snippet below, and note the $(\\color{red}{b}, \\color{red}{w})$ trajectory as we use the gradient to (try to) get to the minimum.\n",
        "2. Adjust the starting values for $\\color{red}{b}$ or $\\color{red}{w}$ or the value of $\\color{blue}{\\eta}$ and see how the resulting trajectory to the minimum changes.\n",
        "3. Can you find a setting for $\\color{blue}{\\eta}$ where things start spiralling out of control and the loss gets bigger and bigger (and not smaller)?\n",
        "4. Can you find a setting for $\\color{blue}{\\eta}$ so that we're still far away from the minimum after `200` parameter update steps?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBhF485-chI2"
      },
      "source": [
        "b = -1                # Change me! Try 2, 7\n",
        "w = -2                # Change me! Try -1, 2\n",
        "learning_rate = 0.1   # Change me! Try 0.01, 0.5, ...\n",
        "\n",
        "parameters_step_list = []\n",
        "\n",
        "for _ in range(200):\n",
        "  parameters_step_list.append([b, w]) \n",
        "  grad_b, grad_w = manual_grad(b, w)\n",
        "  b = b - learning_rate * grad_b\n",
        "  w = w - learning_rate * grad_w\n",
        "\n",
        "plot_loss(parameters_step_list,\n",
        "          \"A loss function, and minimizing it with gradient descent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZTFxmJBdQNM"
      },
      "source": [
        "## **Autodiff: No more manual gradients!**\n",
        "\n",
        "You don't need to do the above calculations yourself thanks to automatic differentiation, or \"autodiff\"!\n",
        "While you can probably derive and code the gradients of the loss function for our linear model without making a mistake somewhere, getting the gradients right for more complex models can be much more work. Much, much more work!\n",
        "Before we introduce autodiff, you should spare a thought for the generations of machine learning researchers who did amazing work without it ;-)\n",
        "\n",
        "Here is autodiff in action in the lines of code below. We contruct a function called `auto_grad`,\n",
        "\n",
        "`auto_grad = jax.grad(loss_function, argnums=(0, 1))`\n",
        "\n",
        "and call it in the same way as we called `manual_grad`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnhN3RJnuyIB"
      },
      "source": [
        "x = np.array(x_data_list)\n",
        "y = np.array(y_data_list)\n",
        "\n",
        "def loss_function(b, w):\n",
        "  f = w * x + b\n",
        "  errors = jnp.abs(y - f)\n",
        "  # Instead of summing over individual data points in a for-loop, and then\n",
        "  # dividing to get the average, we do it in one go. No more for-loops!\n",
        "  return jnp.mean(errors)\n",
        "\n",
        "# This is it! One line of code.\n",
        "auto_grad = jax.grad(loss_function, argnums=(0, 1))\n",
        "\n",
        "# Let's see if it works. Does auto_grad match our manual version?\n",
        "b, w = 2.0, 3.0\n",
        "\n",
        "grad_b, grad_w = auto_grad(b, w)\n",
        "print(\"Autograd         grad_b:\", grad_b, \"  grad_w\", grad_w)\n",
        "\n",
        "grad_b, grad_w = manual_grad(b, w)\n",
        "print(\"Manual gradients grad_b:\", grad_b, \"  grad_w\", grad_w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hH8uDK-2Xkl"
      },
      "source": [
        "## **JAX and other tools**\n",
        "\n",
        "To automatically get the gradients of our loss function `loss_function(b, w)`, we called `jax.grad(loss_function)`.\n",
        "The result was a function `auto_grad(b, w)`, which we could call for any setting of $\\color{red}{b}$ and $\\color{red}{w}$ to get $\\nabla_{\\color{red}{b}} \\mathrm{loss}(\\color{red}{b}, \\color{red}{w})$ and $\\nabla_{\\color{red}{w}} \\mathrm{loss}(\\color{red}{b}, \\color{red}{w})$.\n",
        "This magic happened thanks to JAX.\n",
        "\n",
        "[JAX](https://jax.readthedocs.io/en/latest/index.html) is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n",
        "It does a lot more than automatic differentiation!\n",
        "You'll notice that we didn't use a for-loop like in our illustrative `manual_grad(b, w)` function. We\n",
        "- put all the data points $(x_1, y_1)$ to $(x_N, y_N)$ in vectors `x` and `y`;\n",
        "- computed $\\color{red}{w}$ * `x` + $\\color{red}{b}$ in one line to give a vector `f`, after which `y - f` gave another vector, and `errors = jnp.abs(y - f)` gave the error for each data point in a **vector**;\n",
        "- computed `jnp.mean(errors)`, the average of the errors for each data point.\n",
        "\n",
        "You'll notice `jnp`, short for `jax.numpy`.\n",
        "It is necessary to compile and run your NumPy code on accelerators like GPUs and TPUs!\n",
        "\n",
        "We now minimize the loss function again with gradient descent, this time using the handy automatic gradients that [JAX](https://jax.readthedocs.io/en/latest/index.html) gave us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br3fi0NtC3xW"
      },
      "source": [
        "b, w = -1.0, -2.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "parameters_step_list = []\n",
        "\n",
        "for _ in range(200):\n",
        "  parameters_step_list.append([b, w])\n",
        "  grad_b, grad_w = auto_grad(b, w)      # Now with JAX automatic differentiation\n",
        "  b = b - learning_rate * grad_b\n",
        "  w = w - learning_rate * grad_w\n",
        "\n",
        "plot_loss(parameters_step_list, \"Gradient descent with automatic gradients\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYUxAkBfMcgM"
      },
      "source": [
        "## [Optional] **Analytical solution to linear regression with quadratic loss**\n",
        "Before we move to models that are nonlinear in $x$ we'll show the analytical solution to linear regression under a *mean squared error* loss function.\n",
        "As a reminder, the MSE loss is the sum of quadratic functions\n",
        "\n",
        "$\\mathrm{loss}(\\color{red}{b}, \\color{red}{w}) = \\frac{1}{N} \\sum_{n=1}^N \\big(y_n - (\\color{red}{w} x_n + \\color{red}{b}) \\big)^2$.\n",
        "\n",
        "It is indeed a special case! If we replaced $(y_n - (\\color{red}{w} x_n + \\color{red}{b}) )^2$ with $|y_n - (\\color{red}{w} x_n + \\color{red}{b})|$ -- like we did earlier -- we wouldn't be able to derive a closed-form analytical solution any more.\n",
        "\n",
        "Like we've done before, we will put all data points $(x_n, y_n)$ in vectors, and instead of treating $\\color{red}{b}$ and $\\color{red}{w}$ separately, we'll treat them together as a vector $(\\color{red}{w}, \\color{red}{b})$.\n",
        "We convert to vector notation by defining the following terms. (The advantage of using vector notation is that the same derivation applies to any number of parameters, as we will see later!)\n",
        "\n",
        "$\\color{red}{\\mathbf{w}} =\n",
        "\\begin{pmatrix}\n",
        "    \\color{red}{w} \\\\\n",
        "    \\color{red}{b}\n",
        "\\end{pmatrix} \\ , \\qquad\n",
        "\\mathbf{X} =\n",
        "\\begin{pmatrix}\n",
        "    x_1 & 1 \\\\\n",
        "    x_2 & 1 \\\\\n",
        "    \\vdots & \\vdots\\\\\n",
        "    x_N & 1\n",
        "\\end{pmatrix} \\ , \\qquad\n",
        "\\mathbf{y} =\n",
        "\\begin{pmatrix}\n",
        "    y_1 \\\\\n",
        "    y_2 \\\\\n",
        "    \\vdots \\\\\n",
        "    y_N\n",
        "\\end{pmatrix}$\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "Using this notation, convince yourself that the *mean squared error* loss is equivalent to\n",
        "\n",
        "$\\mathrm{loss}(\\color{red}{\\mathbf{w}}) = \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\color{red}{\\mathbf{w}})^\\mathsf{T}(\\mathbf{y} - \\mathbf{X}\\color{red}{\\mathbf{w}})$.\n",
        "\n",
        "See [this blog post](https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/) for more details.\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "Convince yourself that the MSE loss is minimized at\n",
        "\n",
        "$\\color{red}{\\mathbf{w}^*} = (\\mathbf{X}^\\mathsf{T} \\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T} \\mathbf{y}$\n",
        "\n",
        "For illustrative purposes we'll consider the *mean squared error* and analytical solution below, as knowing the minimum in the convex quadratic case (as we have here) will help us develop intuition around basic gradient descent as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cg7FASxM4M2"
      },
      "source": [
        "# **Non-linear regression**\n",
        "\n",
        "So far we've looked at data that could be fitted fairly accurately with a line. Despite its simplicity, linear regression tends to be very useful in practice, especially as as a starting point in data analysis! However, there are cases where a linear fit is unsatisfying, for example, consider the following dataset in blue with the best linear fit in red:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irUWLQR-fx-3"
      },
      "source": [
        "def generate_wave_like_dataset(min_x=-1, max_x=1, n=100):\n",
        "  xs = np.linspace(min_x, max_x, n)\n",
        "  ys = np.sin(5 * xs) + np.random.normal(size=len(xs), scale=0.2)\n",
        "  return xs, ys\n",
        "\n",
        "def regression_analytical_solution(X, y):\n",
        "  return ((np.linalg.inv(X.T.dot(X))).dot(X.T)).dot(y)\n",
        "\n",
        "def gradient_descent(X, y, learning_rate=1e-2, num_steps=1000):\n",
        "  report_every = num_steps // 10\n",
        "\n",
        "  def loss(current_w, X, y):\n",
        "    y_hat = jnp.dot(X, current_w)\n",
        "    loss = jnp.mean((y_hat - y) ** 2)\n",
        "    return loss, y_hat\n",
        "\n",
        "  loss_and_grad = jax.value_and_grad(loss, has_aux=True)\n",
        "  # Initialize the parameters\n",
        "  w = np.random.normal(size=(X.shape[1]))\n",
        "\n",
        "  # Run a a few steps of gradient descent\n",
        "  for i in range(num_steps):\n",
        "    (loss, y_hat), grad = loss_and_grad(w, X, ys)\n",
        "\n",
        "    if i % report_every == 0:\n",
        "      print(f\"Step {i}: w: {w}, Loss: {loss}, Grad: {grad}\")\n",
        "\n",
        "    w = w - learning_rate * grad\n",
        "\n",
        "  return w\n",
        "\n",
        "def plot_data(y_hat, xs, ys, title):\n",
        "  plt.figure()\n",
        "  plt.scatter(xs, ys, label=\"Data\")\n",
        "  plt.plot(xs, y_hat, 'r', label=title)\n",
        "\n",
        "  plt.title(title)\n",
        "  plt.xlabel(\"Input x\")\n",
        "  plt.ylabel(\"Output y\")\n",
        "  plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN8U2CCqdyjD"
      },
      "source": [
        "xs, ys = generate_wave_like_dataset()\n",
        "X = np.vstack([xs, np.ones(len(xs))]).T\n",
        "w = regression_analytical_solution(X, ys)\n",
        "y_hat = X.dot(w)\n",
        "\n",
        "plot_data(y_hat, xs, ys, \"Linear regression (analytic minimum)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRFCuK2NukOR"
      },
      "source": [
        "The linear regression is clearly missing some important wave-like structure in this data. This is also known as **under-fitting**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn1mr9VWf7Mo"
      },
      "source": [
        "## **From $x$ to a feature vector**\n",
        "\n",
        "Imagine you didn't know that the data generating process was a noisy sine-wave, and instead were given this data by someone else.\n",
        "How would you proceed? One option is to consider increasing the complexity of the linear model by trying to fit a higher order polynomial, for example a 4th degree polynomial:\n",
        "$\\hat{y} = \\color{red}{w_4}x^4 + \\color{red}{w_3}x^3 + \\color{red}{w_2}x^2 + \\color{red}{w_1}x + \\color{red}{w_0}$\n",
        "\n",
        "Luckily we can still solve for the least squares parameters $\\color{red}{w_4}, \\color{red}{w_3}, \\color{red}{w_2}, \\color{red}{w_1}, \\color{red}{w_0}$ using the same techniques we used for fitting a line. \n",
        "\n",
        "Given the dataset $\\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}$, we construct a *feature* matrix $\\mathbf{\\Phi}$ by expending original features, being careful to include terms corresponding to each power of $x$, as follows:\n",
        "\n",
        "$\\mathbf{\\Phi} =\n",
        "\\begin{pmatrix}\n",
        "x_1^4 & x_1^3 & x_1^2 & x_1 & 1 \\\\\n",
        "x_2^4 & x_2^3 & x_2^2 & x_2 & 1 \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "x_n^4 & x_n^3 & x_n^2 & x_n & 1\n",
        "\\end{pmatrix}\n",
        "$\n",
        "\n",
        "And just like before, our $\\mathbf{y}$ vector is $(y_1, y_2, ..., y_n)^\\mathsf{T}$\n",
        "\n",
        "Next we fit a 4th degree polynomial to our data and find that the fit is visually a lot better and captures the wave-like pattern of the data! \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmmeFwxtf71H"
      },
      "source": [
        "def create_data_matrix(xs):\n",
        "  return np.vstack([np.power(xs, 4),\n",
        "                    np.power(xs, 3),\n",
        "                    np.power(xs, 2),\n",
        "                    xs,\n",
        "                    np.ones(len(xs))]).T\n",
        "\n",
        "phi = create_data_matrix(xs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gradient descent with Jax**\n",
        "\n",
        "We fit our fourth degree polynomial using automatic differentiation and gradient descent!"
      ],
      "metadata": {
        "id": "0Og13GIgvuTl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isDluoxlZRLY"
      },
      "source": [
        "w = gradient_descent(phi, ys, num_steps=5000)\n",
        "print(\"w:\", w)\n",
        "y_hat = phi.dot(w)\n",
        "\n",
        "plot_data(y_hat, xs, ys, 'Polynomial regression (gradient descent steps)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Optional] **An analytic solution**\n",
        "\n",
        "Our fourth degree polynomial is still linear in the weights $\\color{red}{\\mathbf{w}}$, and we can fit our polynomial with\n",
        "$\\color{red}{\\mathbf{w}^*} = (\\mathbf{\\Phi}^\\mathsf{T} \\mathbf{\\Phi})^{-1} \\mathbf{\\Phi}^\\mathsf{T} \\mathbf{y}$ as before."
      ],
      "metadata": {
        "id": "CdAKCoRMviyl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iczs-Xlcf9II"
      },
      "source": [
        "w = regression_analytical_solution(phi, ys)\n",
        "print(\"w:\", w)\n",
        "y_hat = phi.dot(w)\n",
        "\n",
        "plot_data(y_hat, xs, ys, \"Polynomial regression (analytic solution)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcvQ5DLTx7yl"
      },
      "source": [
        "## **Can you make the fit better?**\n",
        "\n",
        "Are you pleased with the fit? We certainly aren't! Revisit the code earlier in the Colab, and see if you could adapt the features matrix $\\mathbf{\\Phi}$ to include higher powers than $x^4$. It was coded as:\n",
        "\n",
        "```\n",
        "phi = np.vstack([np.power(xs, 4), np.power(xs, 3),\n",
        "                 np.power(xs, 2), xs, np.ones(len(xs))]).T\n",
        "```\n",
        "\n",
        "If you rerun the cells, do higher powers help? What about:\n",
        "\n",
        "```\n",
        "phi = np.vstack([np.power(xs, 5), np.power(xs, 4), np.power(xs, 3),\n",
        "                 np.power(xs, 2), xs, np.ones(len(xs))]).T\n",
        "```\n",
        "\n",
        "What about other features? What if you included periodic features, and used features like $[1, x, x^2, \\sin(x)]$?\n",
        "\n",
        "```\n",
        "phi = np.vstack([np.sin(xs),\n",
        "                 np.power(xs, 2),\n",
        "                 xs,\n",
        "                 np.ones(len(xs))]).T\n",
        "```\n",
        "\n",
        "In our examples here, we could analytically determine the global minimum $\\color{red}{\\mathbf{w}^*}$. In some of the examples, basic gradient descent took long to get close to $\\color{red}{\\mathbf{w}^*}$. Why do you think that is?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCi33BExZTyd"
      },
      "source": [
        "## **What happens if we extend our predictions out a bit?**\n",
        "\n",
        "*We will now return to the fourth-order polynomial.*\n",
        "\n",
        "In the plot below we fill in some extra data points from the true function (in orange) for comparison, but bear in mind that these were not used to fit the regression model. We are **extrapolating** the model into a previously unseen region!\n",
        "We see that the while the fit looks good in the blue region that the model was fitted on, the fit seems to diverge significantly in the orange region.\n",
        "The model is able to **interpolate** well (fill in gaps in the region it was fitted), but it **extrapolates** (outside the fitted region) poorly.\n",
        "This is a common concern with models in general, unless you can be sure that you have the correct *inductive biases* (assumptions about the data generating process) built into the model, you should be cautious about extrapolating from it.\n",
        "\n",
        "To simplify the explanation, we will use the analytic minimum of the loss function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlB48s5NZUU6"
      },
      "source": [
        "# Recover the analytic solution.\n",
        "phi = create_data_matrix(xs)\n",
        "w = regression_analytical_solution(phi, ys)\n",
        "\n",
        "# Extend the x's and y's.\n",
        "more_xs, more_ys = generate_wave_like_dataset(min_x=-1.2, max_x=-1, n=20)\n",
        "all_xs = np.concatenate([more_xs, xs])\n",
        "all_ys = np.concatenate([more_ys, ys])\n",
        "\n",
        "# Get the design matrix for the extended data, so that we could make predictions\n",
        "# for it.\n",
        "phi = create_data_matrix(all_xs)\n",
        "\n",
        "# Note that we don't recompute w, we use the previously computed values that\n",
        "# only saw x values in the range [0, 10]\n",
        "y_hat = phi.dot(w)\n",
        "\n",
        "plt.scatter(xs, ys, label=\"Data\")\n",
        "plt.scatter(more_xs, more_ys, label=\"Unseen Data\")\n",
        "plt.plot(all_xs, y_hat, 'r', label='Polynomial Regression')\n",
        "\n",
        "plt.title(\"A wave-like dataset with the best-fit line\")\n",
        "plt.xlabel(\"Input x\")\n",
        "plt.ylabel(\"Output y\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyFtpO8jNBEe"
      },
      "source": [
        "# **Generalisation**\n",
        "\n",
        "The previous example you've seen how the red line -- our fourth order polynomial -- does not extrapolate well. It does not generalise well to unseen examples.\n",
        "You've also seen how it interpolates the observed data with varying degrees of success.\n",
        "We will now introduce the themes of under-fitting and over-fitting data.\n",
        "\n",
        "## **Under-fitting**\n",
        "We have already seen an example of **under-fitting**, where we tried to fit a linear model to a noisy sine-wave dataset.\n",
        "The model did not explain the *training data* (the data that was used to fit the model) well, nor would we expect it to *generalise* to any previously unseen data from the same data generating process.\n",
        "Under-fitting is usually the result of a model that is not complex enough to fit the given data.\n",
        "One can detect under-fitting by observing a loss that plateus at a larger than expected value or by using other metrics such as \"r-squared\" from statistics, which measures \"goodness of fit\".\n",
        "As we saw already, the easiest way to address under-fitting is by increasing the complexity of the model, for example by adding more parameters as we did in the case of the 4th degree polynomial.\n",
        "\n",
        "## **Over-fitting**\n",
        "Over-fitting, on the other hand, is the case where the model fits the training data very well but **fails to generalise** to previously unseen data from the same data generating process.\n",
        "This is usually the result of the model having sufficient degrees of freedom to fit the noise in the training data.\n",
        "Let's look at a new dataset, consisting of just 5 points, to illustrate this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5n0ST0HX2ql"
      },
      "source": [
        "np.random.seed(2)\n",
        "\n",
        "xs_small, ys_small = generate_wave_like_dataset(n=5)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(xs_small, ys_small)\n",
        "plt.title(\"A small dataset generated from a line with measurement noise\")\n",
        "plt.xlabel(\"Input x\")\n",
        "plt.ylabel(\"Output y\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8e4fGcAhHq3"
      },
      "source": [
        "Now as we did before, we fit a 4-degree polynomial to this dataset using regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eVK4lZxhIST"
      },
      "source": [
        "phi_small = create_data_matrix(xs_small)\n",
        "w_first = regression_analytical_solution(phi_small, ys_small)\n",
        "\n",
        "xs_plot = np.linspace(-1, 1, 1000)\n",
        "phi_plot = create_data_matrix(xs_plot)\n",
        "y_hat = phi_plot.dot(w_first)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(xs_small, ys_small, label=\"Data\")\n",
        "plt.plot(xs_plot, y_hat, c='r', label=\"Model\")\n",
        "plt.ylim(-4, 4)\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"5 point dataset and regression model fit\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\");\n",
        "plt.show()\n",
        "\n",
        "print(w_first)\n",
        "loss = np.mean(np.power(phi_small.dot(w_first) - ys_small, 2))\n",
        "print(\"Loss:\", loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dxHUCEOX3Ss"
      },
      "source": [
        "As we can see, the model fits the training points very well, passing exactly through each of them. The loss is essentially 0, again indicating that the fit to the training data is extremely good. But will it be that good in practice?\n",
        "\n",
        "Let's plot a few more points from the original function (in orange below) and see how well the model fits these points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHGy5ryJhM-t"
      },
      "source": [
        "xs_more, ys_more = generate_wave_like_dataset(n=32)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(xs_small, ys_small, label=\"Training Data\")\n",
        "plt.scatter(xs_more, ys_more, label=\"Unseen Data\")\n",
        "plt.plot(xs_plot, y_hat, c=\"r\", label=\"Model\")\n",
        "plt.ylim(-4, 4)\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"5 point dataset and regression model fit\")\n",
        "\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\");\n",
        "\n",
        "phi_more = create_data_matrix(xs_more)\n",
        "loss_training = np.mean(np.power(phi_small.dot(w_first) - ys_small, 2))\n",
        "loss_validation = np.mean(np.power(phi_more.dot(w_first) - ys_more, 2))\n",
        "print(\"Loss on training data:\", loss_training)\n",
        "print(\"Loss on unseen data:  \", loss_validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVoBWWUYhQk1"
      },
      "source": [
        "The loss on this previously unseen data is much larger than on the training data.\n",
        "This is a sign of over-fitting.\n",
        "The model fits the training data very well but fails to generalise to previously unseen data, *even in close vicinity to the training data*.\n",
        "\n",
        "## **What shall we do? Pause here!**\n",
        "\n",
        "Before progressing with this practical, take a moment to think about the problem. In machine learning, there are many practical approaches to getting a model that generalizes well. As you can guess, much theory is devoted to the problem too!\n",
        "\n",
        "With what you've seen so far, try to explain to your neighbour\n",
        "\n",
        "1.   every factor that you can think of, that could cause a model to generalize poorly;\n",
        "2.   some ideas that you could think of to improve the model's fit to (unseen) data;\n",
        "3.   any underlying assumptions that you are making about unseen data.\n",
        "\n",
        "Don't proceed until you've had a solid discussion on the topic. If someone is tutoring this practical, they might contribute to the discussion!\n",
        "\n",
        "We shall now look at some approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0pM8Kbqq7g7"
      },
      "source": [
        "## **Train, validate, test**\n",
        "\n",
        "Before we implement solutions for overfitting, we first introduce a technique for detecting it based on the idea of the loss being low on the training data and higher on unseen data.\n",
        "This technique is known as **model selection**.\n",
        "We split our data into two sets, a **training set** and a **validation set**.\n",
        "\n",
        "We will use gradient descent to fit the model parameters.\n",
        "While fitting the model to the training set, we periodically evaluate it on the validation set.\n",
        "While training, the loss initiallly decreases on both the training set and the validation set.\n",
        "However, after some number of training steps, the loss may start increasing on the validation set while continuing to decrease on the training set.\n",
        "It is an indication that our model's performance on unseen data will also get worse, even though our training curves look great.\n",
        "There are more general procedures to create many subsamples of training data and validation data pairs for small datasets and this technique is called **cross-validation**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soeEujlGhSjw"
      },
      "source": [
        "def gradient_descent_with_validation(phi_train, y_train, phi_val, y_val,\n",
        "                                     learning_rate=1e-1, num_steps=1000):\n",
        "  def loss(current_w, phi, y):\n",
        "    y_hat = jnp.dot(phi, current_w)\n",
        "    loss = jnp.mean((y_hat - y) ** 2)\n",
        "    return loss, y_hat\n",
        "\n",
        "  loss_and_grad = jax.value_and_grad(loss, has_aux=True)\n",
        "\n",
        "  # Initialize the parameters\n",
        "  w =  0.2 * np.random.normal(size=(phi_train.shape[1]))\n",
        "\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  # Run a few steps of gradient descent\n",
        "  for i in range(num_steps):\n",
        "    (train_loss, y_hat), grad = loss_and_grad(w, phi_train, y_train)\n",
        "    (val_loss, y_hat), _ = loss_and_grad(w, phi_val, y_val)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    w = w - learning_rate * grad\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(range(len(train_losses)), train_losses, label=\"Train\")\n",
        "  plt.plot(range(len(val_losses)), val_losses, label=\"Validation\")\n",
        "  plt.title(\"Train and validation loss curves\")\n",
        "  plt.xlabel(\"Step\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend()\n",
        "\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_lsXzV6hTu_"
      },
      "source": [
        "np.random.seed(2)\n",
        "\n",
        "n=10\n",
        "x_all, y_all = generate_wave_like_dataset(n=n)\n",
        "\n",
        "# In practice, a smaller preportion of data is used for validation. The example\n",
        "# below is for illustrative purposes only. \n",
        "training_set = np.random.permutation(n) < 0.5 * n\n",
        "x_train = x_all[training_set]\n",
        "y_train = y_all[training_set]\n",
        "phi_train = create_data_matrix(x_train)\n",
        "\n",
        "x_val = x_all[~training_set]\n",
        "y_val = y_all[~training_set]\n",
        "phi_val = create_data_matrix(x_val)\n",
        "\n",
        "w_gd_val = gradient_descent_with_validation(phi_train, y_train, phi_val, y_val)\n",
        "\n",
        "y_hat = phi_plot.dot(w_gd_val)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x_train, y_train, label=\"Training Data\")\n",
        "plt.scatter(x_val, y_val, label=\"Validation Data\")\n",
        "plt.plot(xs_plot, y_hat, c=\"r\", label=\"Model\")\n",
        "plt.ylim(-4, 4)\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Train and validation dataset with regression model fit\")\n",
        "\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you look at the train and validation loss curves above, you'll notice a point where the validation error becomes bigger again, even though the training error continues to shrink.\n",
        "Shall we stop our training process there, and not minimize the training loss exactly?\n",
        "It is a reasonable practical suggestion, as the validation error tells us something about the model overfitting the data."
      ],
      "metadata": {
        "id": "w4GYRxwDzVDK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjaAqVUGhWSs"
      },
      "source": [
        "### **Early stopping**\n",
        "\n",
        "\n",
        "\n",
        "This technique of stopping training when the validation loss is minimised can also be automated in your training script by monitoring the validation loss and stopping as soon as it starts increasing. This technique is known as **early stopping** and is a form of model **regularization**.\n",
        "\n",
        "Let's now stop training at around 200 training steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej1iNwbKn7GV"
      },
      "source": [
        "w_early_stopping = gradient_descent_with_validation(\n",
        "    phi_train, y_train, phi_val, y_val, num_steps=200)\n",
        "\n",
        "y_hat = phi_plot.dot(w_early_stopping)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x_train, y_train, label=\"Training Data\")\n",
        "plt.scatter(x_val, y_val, label=\"Validation Data\")\n",
        "plt.plot(xs_plot, y_hat, c=\"r\", label=\"Model\")\n",
        "plt.ylim(-4, 4)\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Train and validation dataset with regression model fit\")\n",
        "\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4ll3wFrNE6-"
      },
      "source": [
        "### **Regularisation**\n",
        "\n",
        "Regularisation refers to a set of techniques that are used to control and prevent overfitting in machine learning models.\n",
        "We've already seen an example of regularisation in the form of **early stopping**.\n",
        "\n",
        "We'll find a clue to why the model with parameter vectors we found by early stopping generalized better than the first model that overfit.\n",
        "Let's compare their parameter vectors next.\n",
        "The magnitudes of the individual components, as well as the L2-norm (the square root of sum of squares of each magnitude of vector) are smaller for our \"early stopped\" model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYYkTlDPn-pj"
      },
      "source": [
        "print(\"First model parameters:\", w_first)\n",
        "print(\"First model parameter L2-norm:\", np.linalg.norm(w_first, 2))\n",
        "print()\n",
        "print(\"Early stopping model parameters:\", w_early_stopping)\n",
        "print(\"Early stopping model parameter L2-norm:\", np.linalg.norm(w_early_stopping, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErIhIr2WIrxc"
      },
      "source": [
        "Notice how the magnitude (as measured by the L2 norm) of the first parameter vector is significantly larger than the magnitude of the early stopping parameter vector.\n",
        "It is often the case that a model that overfits has a larger magnitude parameter vector than one that doesn't.\n",
        "A well-known and principled way of controlling the magnitude of the parameter vector is to add a \"regularization team\" to the objective function.\n",
        "This term discourages weight vectors to have large magnitudes.\n",
        "\n",
        "In particular, we add an\n",
        "**L2-regularisation term** $\\|\\color{red}{\\mathbf{w}}\\|_2^2$ -- which is scaled by a coefficient $\\lambda > 0$ -- to the \n",
        "linear regression loss function that we encountered earlier,\n",
        "$\\mathrm{loss}(\\color{red}{\\mathbf{w}}) =  \\frac{1}{N} (\\mathbf{y} - \\mathbf{\\Phi}\\color{red}{\\mathbf{w}})^\\mathsf{T}(\\mathbf{y} - \\mathbf{\\Phi}\\color{red}{\\mathbf{w}})$.\n",
        "Our objective is then to minimize a new loss function\n",
        "\n",
        "$\\underset{\\color{red}{\\mathbf{w}}}{\\text{minimize}} \\\n",
        "\\frac{1}{N} (\\mathbf{y} - \\mathbf{\\Phi}\\color{red}{\\mathbf{w}})^\\mathsf{T}(\\mathbf{y} - \\mathbf{\\Phi}\\color{red}{\\mathbf{w}})\n",
        "+ \\lambda \\|\\color{red}{\\mathbf{w}}\\|_2^2$\n",
        "\n",
        "with respect to parameters $\\color{red}{\\mathbf{w}}$, where $\\|\\color{red}{\\mathbf{w}}\\|_2^2 = \\sum_{j} \\color{red}{w_j}^2$.\n",
        "\n",
        "The optimizer now has to *trade off* improving the fit to the training data (decrease the first term) with the increase in magnitude of the parameter vector (increase in the second term).\n",
        "The *hyperparameter* $\\lambda$ controls the strength of this trade-off and is something that you as a user can tune.\n",
        "We can tune this by training a few values for this hyperparameter on the training set (it is called **hyperparameter sweep**) and pick the value that gives the smallest error on the validation set.\n",
        "\n",
        "There is an analytical solution to this optimization problem:\n",
        "\n",
        "$\\color{red}{\\mathbf{w}^*} = (\\mathbf{\\Phi}^\\mathsf{T} \\mathbf{\\Phi} + \\lambda \\mathbf{I})^{-1} \\mathbf{\\Phi}^\\mathsf{T} \\mathbf{y}$\n",
        "\n",
        "Notice that only difference from the least squares solution is that we add $\\lambda \\mathbf{I}$ to $\\mathbf{\\Phi}^\\mathsf{T} \\mathbf{\\Phi}$; see **Appendix B** for a derivation.\n",
        "\n",
        "Now we can practice how to perform regularized regression for our data. Let's start coding and running the experiments!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StZLbXhnSo0w"
      },
      "source": [
        "# Adopted the function regression_analytical_solution with a regularizer\n",
        "def regularized_regression_analytical_solution(phi, y, regularization_coef):\n",
        "  t = phi.shape[0]\n",
        "  return (np.linalg.inv(phi.T.dot(phi) +\n",
        "                        regularization_coef * np.identity(t)\n",
        "                        ).dot(phi.T)).dot(y)\n",
        "\n",
        "# Put some values for the hyperparameter lambda to an array: these are our\n",
        "# hyperparameter values to sweep\n",
        "regularization_coefs = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2]\n",
        "min_validation_error = np.Inf\n",
        "\n",
        "# Accumulate training errors in a list\n",
        "training_errors = []\n",
        "# Accumulate validation errors in a list\n",
        "validation_errors = []\n",
        "\n",
        "# Train the model for each hyperparameter value and calculate the corresponding\n",
        "# the validation error for each\n",
        "for regularization_coef in regularization_coefs:\n",
        "  w_reg = regularized_regression_analytical_solution(phi_train, y_train,\n",
        "                                                     regularization_coef)\n",
        "  training_error = jnp.mean((jnp.dot(phi_train, w_reg) - y_train) ** 2)\n",
        "  training_errors.append(training_error)\n",
        "  validation_error = jnp.mean((jnp.dot(phi_val, w_reg) - y_val) ** 2)\n",
        "  validation_errors.append(validation_error)\n",
        "\n",
        "  # Find the hyperparameter value that gives the best validation error\n",
        "  if validation_error < min_validation_error:\n",
        "    min_validation_error = validation_error\n",
        "    best_regularization_coef = regularization_coef\n",
        "    best_w_reg = w_reg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIAndqwKSuHw"
      },
      "source": [
        "We just performed a basic form of cross-validation that has one training and validation set to pick best hyperparameter value.\n",
        "Let's check best $\\lambda$ value  and magnitudes of the weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YFtekFQSxOx"
      },
      "source": [
        "print(\"Best value for hyperparameter alpha:\", best_regularization_coef)\n",
        "print(\"Weights for the regularized regression:\", best_w_reg)\n",
        "print(\"L2-norm for the weights for the regularized regression:\",\n",
        "      np.linalg.norm(best_w_reg, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlBdulW1S03t"
      },
      "source": [
        "We collected training error and validation error values for each regularization coefficient value, $\\lambda$.\n",
        "You can see in the plot below that when we increase $\\lambda$, the training error increases while validation error decreases.\n",
        "This means that in the beginning, where we have small values of $\\lambda$, the model was overfitting; thus, regularizing more by increasing the coefficient helps. However, as we further increase $\\lambda$, it starts to underfit as both training and validation error increases.\n",
        "Therefore, we applied the cross validation to find the best value: we loop over different hyperparameter values, train and log the validation error and find the best hyperparameter value that gives the minumum validation error.  You will see in next colabs that we will have other hyperparameters for different models and we will apply this technique to pick the best values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1sVOMkJS3CV"
      },
      "source": [
        "plt.figure()\n",
        "plt.semilogx(np.array(regularization_coefs), np.array(training_errors), label=\"Train\")\n",
        "plt.semilogx(np.array(regularization_coefs), np.array(validation_errors), label=\"Validation\")\n",
        "plt.title(\"Train and validation losses vs regularization coefficient\")\n",
        "plt.xlabel(\"Regularization coefficient $\\lambda$\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-2qHAAvS526"
      },
      "source": [
        "We reached a different solution with early stopping than we did by adding a regularizer.\n",
        "The analytical solution was convenient as we didn't have to worry whether the optimizer would converge to a (global) minimum. But optimization is not always that convenient!\n",
        "The models we use for the more complicated problems\n",
        "usually can't be analytically minimized,\n",
        "and we normally use optimizers like gradient descent based algorithms, possibly with early stopping or regularizers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N3NkTNUoAbY"
      },
      "source": [
        "\n",
        "### [Optional] **Ridge and Lasso**\n",
        "The special case of linear regression with an L2 regularisation is also known in the statistics community as **Ridge Regression**.\n",
        "Ridge regression has a tendency to shrink the parameters in a paramater vector, but not necessarily to set them all the way to zero.\n",
        "Replacing the L2-norm in the regularised loss with an L1 norm leads to another model called **The Lasso**.\n",
        "This has the tendency to set components of the parameter vector to 0 and can therefore also be used as a *variable selection technique*.\n",
        "For example, suppose you are trying to predict house prices and you have 10 different inputs you could use to predict the price. However, you are unsure which (if any) of these 10 inputs is actually related to the house price. You could construct a Lasso model with all 10 inputs and after fitting it, find that it sets the parameter associated with 5 of these inputs to 0. This would suggest that those inputs were not useful in predicting the output house price.\n",
        "If you're interested in finding out more, including why Ridge regression shrinks parameters and Lasso sets them to zero, see [this resource](https://online.stat.psu.edu/stat508/lesson/5).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVbvH0bkZXS1"
      },
      "source": [
        "# **Appendices**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHyT_cyLZa8t"
      },
      "source": [
        "## **Appendix A: When gradient descent goes wrong**\n",
        "Lets re-visit a small variation of the wave-like dataset from earlier and attempt to fit a 4th degree polynomial to it using gradient descent.\n",
        "You'll notice from the output that the loss starts off very large and seems to increase at every step before becoming \"infinity\"! What's going on here?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecmubishZaoH"
      },
      "source": [
        "xs, ys = generate_wave_like_dataset(n=100)\n",
        "xs = 5 * (xs + 1)\n",
        "phi = create_data_matrix(xs)\n",
        "\n",
        "w = regression_analytical_solution(phi, ys)\n",
        "y_hat = phi.dot(w)\n",
        "\n",
        "plot_data(y_hat, xs, ys, \"Linear regression (analytic minimum)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLGliUN2ZeDQ"
      },
      "source": [
        "gradient_descent(phi, ys, num_steps=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r603LZ_vZcZq"
      },
      "source": [
        "The first issue seems to be that the magnitude of the gradient is large.\n",
        "This means that at each gradient descent step, the change to the parameters $\\color{red}{\\mathbf{w}}$ are extremely large, causing $\\color{red}{\\mathbf{w}}$ to overshoot and go to a worse range in terms of loss.\n",
        "\n",
        "The following illustrates how this may happen in the simple case of a single parameter $\\color{red}{w}$, a mean squared error loss function $l(\\color{red}{w})$, plotted in blue, and an initial value $\\color{red}{w}^{(0)} = 2.5$ shown as the single red dot where the value of the loss is $2.5^2 = 6.25$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2m9gQqhZgmR"
      },
      "source": [
        "ws = np.linspace(-10, 10, 1000)\n",
        "losses = ws ** 2\n",
        "\n",
        "plt.plot(ws, losses)\n",
        "plt.title(\"Loss function for a single parameter w\")\n",
        "plt.xlabel(\"$w$\")\n",
        "plt.ylabel(\"$l(w)$\")\n",
        "\n",
        "points = [(2.5, 2.5**2)]\n",
        "\n",
        "ws, losses = list(zip(*points))\n",
        "plt.scatter(ws, losses, c='r', s=50, zorder=100)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdepdFl1ZiJS"
      },
      "source": [
        "The gradient of this loss function is simply $l'(\\color{red}{w}) = 2\\color{red}{w}$. Suppose now that our learning rate was 1.5. If we take a gradient step, we arrive at a new w value of:\n",
        "\n",
        "$\\color{red}{w}^{(1)} = \\color{red}{w}^{(0)} - 1.5 \\cdot 2\\color{red}{w}^{(0)} = 2.5 - 7.5 = -5$\n",
        "\n",
        "At $\\color{red}{w}^{(1)} = -5$, the value of the loss value is now $(-5)^2 = 25$! This point and 2 more steps of gradient descent are illustrated below. We see that the loss keeps increasing and the parameter $\\color{red}{w}$ moves further away from the minimum. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZLCa5wVZjqj"
      },
      "source": [
        "LEARNING_RATE = 1.5\n",
        "\n",
        "ws = np.linspace(-20, 20, 1000)\n",
        "losses = ws ** 2\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "plt.plot(ws, losses)\n",
        "plt.title(\"Loss function for a single parameter w\")\n",
        "plt.xlabel(\"$w$\")\n",
        "plt.ylabel(\"$l(w)$\")\n",
        "\n",
        "w = 2.5\n",
        "points = []\n",
        "for _ in range(4):\n",
        "  points.append((w, w**2))\n",
        "  w = w - LEARNING_RATE * 2 * w\n",
        "\n",
        "ws, losses = list(zip(*points))\n",
        "plt.scatter(ws, losses, c='r', s=50, zorder=10)\n",
        "\n",
        "for point, next_point in zip(points, points[1:]):\n",
        "  ax.annotate(\"\", xy=next_point, xytext=point,\n",
        "              arrowprops=dict(arrowstyle=\"-|>\", ls=\"--\", color=\"grey\"))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPvqEAEbZlQd"
      },
      "source": [
        "One way to address this is to use a lower learning rate, with a learning rate of 0.3 and 5 steps of gradient descent, the picture looks quite different. This time the loss decreases with every step and we arrive at the minimum loss where $\\color{red}{w}=0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "038XCKEZZmhd"
      },
      "source": [
        "LEARNING_RATE = 0.3\n",
        "\n",
        "ws = np.linspace(-5, 5, 1000)\n",
        "losses = ws ** 2\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "plt.plot(ws, losses)\n",
        "plt.title(\"Loss function for a single parameter w\")\n",
        "plt.xlabel(\"$w$\")\n",
        "plt.ylabel(\"$l(w)$\")\n",
        "\n",
        "w = 2.5\n",
        "points = []\n",
        "for _ in range(4):\n",
        "  points.append((w, w**2))\n",
        "  w = w - LEARNING_RATE * 2 * w\n",
        "\n",
        "ws, losses = list(zip(*points))\n",
        "plt.scatter(ws, losses, c='r', s=50, zorder=10)\n",
        "\n",
        "for point, next_point in zip(points, points[1:]):\n",
        "  ax.annotate(\"\", xy=next_point, xytext=point,\n",
        "              arrowprops=dict(arrowstyle=\"-|>\", ls=\"--\", color=\"grey\"))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys4WYTyBZoJW"
      },
      "source": [
        "Let's try setting a lower learning rate with our earlier example.\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "Try settings some different learning rates in the cell below to see what happens.\n",
        "With a learning rate as low as 1e-8, the loss doesn't explode, but decreases extremely slowly.\n",
        "We will eventually get to the optimal value but it will take a long time (over 100 000 steps!).\n",
        "\n",
        "The issue seems to be that when we're far away from the optimal setting of the parameters $\\color{red}{\\mathbf{w}}$, the gradient magnitude is extremely large, so we set a lower learning rate to compensate.\n",
        "However, when we get into a better range for the paramters, the gradient magnitude shrinks into a more manageable range, but now our learning rate is so small that we need to do thousands of tiny steps to reach our goal. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W-f2Wp2ZpdT"
      },
      "source": [
        "LEARNING_RATE = 1e-8\n",
        "\n",
        "w = gradient_descent(phi, ys, num_steps=100, learning_rate=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLAew3huZqqP"
      },
      "source": [
        "### **Optimization!**\n",
        "\n",
        "By now, you can guess that **optimization** is a big and important research area.\n",
        "Surely, there must be cleverer things that we could do than basic gradient descent. We've shown how we can run into serious difficulty with an apparently very simple problem.\n",
        "\n",
        "There are much, much cleverer things that we can do! We will leave you with this last open-ended exercise:\n",
        "Can you reimplement a better version of \n",
        "\n",
        "```\n",
        "def gradient_descent(X, y, ...):\n",
        "  ...\n",
        "\n",
        "  for i in range(num_steps):\n",
        "    _, grad = loss_and_grad(w, X, ys)\n",
        "    \n",
        "    w = w - learning_rate * grad\n",
        "  \n",
        "  return w\n",
        "```\n",
        "\n",
        "following some of the approaches in [this blog](https://ruder.io/optimizing-gradient-descent/index.html)?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW342u5PXwKM"
      },
      "source": [
        "## **Appendix B: Linear regression analytical solution**\n",
        "Using the vector notation we introduced previously, we can calculate summation of the squared loss over data points as follows: \n",
        "\n",
        "\\begin{align}\n",
        "\\newcommand{\\T}{\\mathsf{T}}\n",
        "\\newcommand{\\y}{\\mathbf{y}}\n",
        "\\newcommand{\\X}{\\mathbf{X}}\n",
        "\\newcommand{\\w}{\\color{red}{\\mathbf{w}}}\n",
        "l(\\w) &= \\frac{1}{N} (\\y - \\X\\w)^\\T(\\y - \\X\\w) & (1) \\\\\n",
        "&= \\frac{1}{N} (\\y^\\T - (\\X\\w)^\\T)(\\y - \\X\\w) & (2) \\\\\n",
        "&= \\frac{1}{N} [\\y^\\T\\y - \\y^\\T(\\X\\w) - (\\X\\w)^\\T\\y + (\\X\\w)^\\T(\\X\\w)] & (3)  \\\\\n",
        "&= \\frac{1}{N}[\\y^\\T\\y - 2(\\X\\w)^\\T \\y + (\\X\\w)^\\T(\\X\\w)] & (4)  \\\\\n",
        "&= \\frac{1}{N}[\\y^\\T\\y - 2(\\X\\w)^\\T \\y + \\w^\\T\\X^\\T\\X\\w] & (5)\n",
        "\\end{align}\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "Convince yourself that all the steps make sense,\n",
        "1.  using the matrix-transpose identity $(A+B)^\\mathsf{T} = A^\\mathsf{T} + B^\\mathsf{T}$;\n",
        "2.  multiplying terms;\n",
        "3.  $(\\X\\w)$ and $\\y$ are vectors, so the order of multiplication doesn't matter;\n",
        "4.  using the identity $(AB)^\\T = B^\\T A^\\T$.\n",
        "\n",
        "Now we can calculate the derivative of the loss with respect to the parameter vector $\\mathbf{w}$.\n",
        "Using matrix-vector derivatives from [matrixcheatsheet](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf)  providing related parts from [matrixcookbook](https://www2.imm.dtu.dk/pubdb/edoc/imm3274.pdf), we get:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\delta l}{\\delta \\w} & = -2\\X^\\T \\y + 2\\X^\\T\\X\\w &  (6)\n",
        "\\end{align}\n",
        "\n",
        "Finally, thanks to special properties of loss (a convex quadratic function), setting this derivative equal to zero and solving for $\\w$ gives the optimum value $\\w^{\\color{red}{*}}$ which is the minimim of the squared loss:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\delta l}{\\delta \\w} = -2\\X^\\T \\y + 2\\X^\\T\\X\\w^{\\color{red}{*}} & = 0 \\\\\n",
        "\\Rightarrow \\X^\\T \\y & = \\X^\\T\\X\\w^{\\color{red}{*}} \\\\\n",
        "\\Rightarrow \\w^{\\color{red}{*}} & = (\\X^\\T\\X)^{-1} \\X^\\T \\y \\\\\n",
        "\\end{align}\n",
        "\n",
        "We can use the above derivative (6) to calculate the derivative of the sum of the squared loss and L2-norm regularization (that we covered in Regression section above)\n",
        "and again find the parameter value that makes the derivative zero:\n",
        "\\begin{align}\n",
        "-2\\X^\\T \\y + 2\\X^\\T\\X\\w^{\\color{red}{*}} + 2 \\lambda \\w^{\\color{red}{*}} & = 0 \\\\\n",
        "\\Rightarrow \\X^\\T \\y & = \\X^\\T\\X\\w^{\\color{red}{*}} + \\lambda \\w^{\\color{red}{*}}\\\\\n",
        "\\Rightarrow \\X^\\T \\y & = (\\X^\\T\\X+ \\lambda \\mathbf{I})\\w^{\\color{red}{*}}\\\\\n",
        "\\Rightarrow \\w^{\\color{red}{*}} & = (\\X^\\T\\X + \\lambda \\mathbf{I})^{-1} \\X^\\T \\y \\\\\n",
        "\\end{align}\n",
        "\n",
        "Not all problems have an analytical solution; for those problems we use methods like gradient descent to find the optimum.\n",
        "That is why we explained how to apply gradient descent based training algorithm to find the minimum of a squared loss fuction, even though in this case, we could calculate it directly."
      ]
    }
  ]
}
